<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Deep Learning on Jay Tech</title>
    <link>//localhost:1313/tags/deep-learning/</link>
    <description>Recent content in Deep Learning on Jay Tech</description>
    <image>
      <title>Jay Tech</title>
      <url>//localhost:1313/assets/images/profile.png</url>
      <link>//localhost:1313/assets/images/profile.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 23 May 2024 20:31:53 +0800</lastBuildDate>
    <atom:link href="//localhost:1313/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>3DGS Tutorial</title>
      <link>//localhost:1313/posts/3dgs/</link>
      <pubDate>Thu, 23 May 2024 20:31:53 +0800</pubDate>
      <guid>//localhost:1313/posts/3dgs/</guid>
      <description>3DGS åŸºæœ¬æ€æƒ³ 3Dé«˜æ–¯åˆ†å¸ƒå¯ä»¥é€šè¿‡å®ƒä»¬çš„å„å‘å¼‚æ€§åæ–¹å·®çŸ©é˜µã€ä½ç½®å’Œé€æ˜åº¦ç­‰å‚æ•°æ¥æœ‰æ•ˆåœ°è¡¨ç¤ºå¤æ‚åœºæ™¯ã€‚ç”±äºè¿™äº›å‚æ•°æ˜¯é€šè¿‡æœºå™¨å­¦ä¹ æ–¹æ³•è¿›è¡Œè®­ç»ƒçš„ï¼Œæ¸²æŸ“é˜¶æ®µæ— éœ€è¿›è¡Œå¤§é‡å¤„ç†ã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥åˆ©ç”¨åŸºäºç“¦ç‰‡çš„å…‰æ …åŒ–å™¨å®ç°å¿«é€Ÿæ¸²æŸ“ï¼Œä»è€Œåœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—çš„æå‡ã€‚ åˆ›æ–°ç‚¹ Point-Based Renderingï¼šç‚¹åŸºæ¸²æŸ“ç›´æ¥å°†ä¸‰ç»´ç©ºé—´ä¸­çš„ç‚¹æ¸²æŸ“ä¸ºå›¾åƒã€‚ Tiled Rasterizationï¼šåˆ†å—å…‰æ …åŒ–çš„åŸºæœ¬æ€æƒ³æ˜¯å°†å±å¹•åˆ’åˆ†ä¸ºå¤šä¸ªå°å—ï¼ˆTilesï¼‰ï¼Œç„¶ååœ¨æ¯ä¸ªå°å—å†…è¿›è¡Œç›¸å…³è®¡ç®—å’Œå¤„ç†ï¼ˆå¯å¾®åˆ†ï¼‰ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—å‡å°‘å†…å­˜æµé‡ï¼Œä»è€Œæé«˜æ¸²æŸ“æ•ˆç‡ã€‚ Spherical Harmonicsï¼šçƒè°å‡½æ•°æ˜¯ä¸€ç§åœ¨çƒé¢ä¸Šè¡¨ç¤ºå‡½æ•°çš„æ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºæè¿°çƒå½¢è¡¨é¢çš„å…‰ç…§å’Œé˜´å½±æ•ˆæœã€‚ åŸºæœ¬æµç¨‹ æ”¶é›†æ•°æ®
å›¾åƒ
è§†é¢‘-&amp;gt;ffmpegæˆªå–è§†é¢‘å¸§
ffmpeg -i &amp;lt;VIDEO_PATH&amp;gt; -qscale:v 1 -qmin 1 -vf fps=2 %04d.jpg è¾“å‡ºå¦‚ä¸‹
ğŸ“¦ $FOLDER_PATH â”£ ğŸ“‚ input â”ƒ â”£ ğŸ“œ 000000.jpg â”ƒ â”£ ğŸ“œ 000001.jpg â”ƒ â”£ ğŸ“œ ... è·å–ç›¸æœºä½å§¿
COLMAPï¼šå¼€æºStructure-from-Motion (SfM) è½¯ä»¶ï¼Œè¾“å…¥imagesï¼Œè¾“å‡ºç›¸æœºä½å§¿
åŸè®ºæ–‡ä½¿ç”¨çš„æ˜¯è‡ªå¸¦çš„convert.pyï¼Œè‡ªåŠ¨è°ƒç”¨COLMAPå¹¶è½¬æ¢æˆéœ€è¦çš„æ ¼å¼
æ¡Œé¢è½¯ä»¶ï¼šRealityCapture, Metashape
ç§»åŠ¨appï¼šPolycam, Record3Dï¼ˆåˆ©ç”¨äº†é›·è¾¾ï¼‰
è¾“å‡ºå¦‚ä¸‹ï¼š
ğŸ“¦ $FOLDER_PATH â”£ ğŸ“‚ (input) â”£ ğŸ“‚ (distorted) â”£ ğŸ“‚ images â”£ ğŸ“‚ sparse â”ƒ â”£ ğŸ“‚ 0 â”ƒ â”ƒ â”£ ğŸ“œ points3D.</description>
    </item>
    <item>
      <title>Recent Advances in Vision Foundation Models</title>
      <link>//localhost:1313/posts/recent-advances-in-vision-foundation-models.assets/</link>
      <pubDate>Thu, 28 Mar 2024 13:30:42 +0800</pubDate>
      <guid>//localhost:1313/posts/recent-advances-in-vision-foundation-models.assets/</guid>
      <description>åŸæ–‡é“¾æ¥
Q1: how to learn image representations? Overview æ”¹è¿› CLIP æ•°æ®å±‚é¢ï¼šData scaling up æ¨¡å‹å±‚é¢ï¼šModel design image side
FLIPï¼ˆScaling CLIP training via maskingï¼‰ï¼šæ˜¯ä¸€ç§æ”¹è¿›çš„è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºæé«˜ CLIPæ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚FLIP çš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºé®æŒ¡å›¾åƒçš„éƒ¨åˆ†åŒºåŸŸï¼Œåªå¯¹å¯è§çš„åŒºåŸŸè¿›è¡Œç¼–ç ã€‚
Scaling language-image pre-training via masking, CVPR 2023
language side
K-Lite: å°†å¤–éƒ¨çŸ¥è¯†èå…¥åˆ°å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒä¸­ï¼Œåœ¨ K-Lite ä¸­ï¼Œå®ä½“çš„ç»´åŸºç™¾ç§‘å®šä¹‰ï¼ˆknowledgeï¼‰å¯ä»¥ä¸åŸå§‹çš„å›¾åƒæ›¿ä»£æ–‡æœ¬ï¼ˆalt-textï¼‰ä¸€èµ·è‡ªç„¶åœ°ç”¨äºå¯¹æ¯”é¢„è®­ç»ƒã€‚
K-lite: Learning transferable visual models with external knowledge, NeurIPS 2022
improved interpretability
STAIRï¼ˆLearning Sparse Text and Image Representation in Grounded Tokensï¼‰ï¼š å°†å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°é«˜ç»´ç¨€ç–åµŒå…¥ç©ºé—´ï¼› æ¯ä¸ªç»´åº¦çš„å€¼æ˜¯ä¸€ä¸ªéè´Ÿæ ‡é‡ï¼Œè¡¨ç¤ºä¸è¯¥ç»´åº¦å¯¹åº”çš„è¯æˆ–æ ‡è®°çš„æƒé‡ï¼› æä¾›äº†æ›´å¥½çš„æ€§èƒ½å’Œæ›´æ¸…æ™°åœ°è¯†åˆ«å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼› STAIR: Learning Sparse Text and Image Representation in Grounded Tokens, 2023</description>
    </item>
  </channel>
</rss>
