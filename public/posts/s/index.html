<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Jay Tech</title>
<meta name="keywords" content="">
<meta name="description" content="原文链接 [图片] Q1: how to learn image representations? Overview [图片] [图片] 改进 CLIP 数据层面：Data scaling up [图片] 模型层面：Model design
image side FLIP（Scaling CLIP training via masking）：是一种改进的训练方法，用于提高 CLIP模型的训练效率。FLIP 的核心思想是在训练过程中随机遮挡图像的部分区域，只对可见的区域进行编码。 Scaling language-image pre-training via masking, CVPR 2023 [图片] language side K-Lite: 将外部知识融入到对比学习预训练中，在 K-Lite 中，实体的维基百科定义（knowledge）可以与原始的图像替代文本（alt-text）一起自然地用于对比预训练。 K-lite: Learning transferable visual models with external knowledge, NeurIPS 2022 [图片] improved interpretability STAIR（Learning Sparse Text and Image Representation in Grounded Tokens）： 将图像和文本映射到高维稀疏嵌入空间； 每个维度的值是一个非负标量，表示与该维度对应的词或标记的权重； 提供了更好的性能和更清晰地识别图像和文本之间的对应关系； STAIR: Learning Sparse Text and Image Representation in Grounded Tokens, 2023 [图片] more modalities ImageBind（One embedding space to bind them all）：将不同模态（在这篇论文中有7种模态）链接到一个共同空间的方法。 一个共同的嵌入空间。 使用冻结的预训练 CLIP。 学习其他模态编码器：对于每个非图像和文本的模态，ImageBind 学习一个编码器，将该模态的数据映射到 CLIP 的嵌入空间中。 [图片] 目标函数：Objective function fine-grained supervision（细粒度监督） FILIP：通过细粒度监督来改进视觉-语言预训练的方法。即单词-图像块（patch）对齐。 双编码器结构：FILIP 仍然使用双编码器结构，即分别对图像和文本进行编码，而不是使用融合编码器。 细粒度监督：FILIP 的核心在于细粒度的监督。它首先计算单词和图像块之间的相似性，然后通过最大池化（max pooling）聚合这个相似性矩阵来计算损失。这种方法允许模型学习更细粒度的图像-文本对齐。 学习单词-图像块对齐：通过这种细粒度的相似性计算和损失函数，FILIP 能够学习单词和图像块之间的对齐关系。这种对齐对于可视化非常有用，因为它提供了图像和文本之间更明确的关联。 FILIP: Fine-grained Interactive Language-Image Pre-Training, ICLR 2022 [图片] adding a generative branch Coca：通过在预训练阶段同时利用图像-文本对和图像-标签对来学习图像和文本的表征。并增加了一个生成分支，以提升模型性能并赋予其新的能力，如图像描述和视觉问答。 对比学习：CoCa 使用对比学习机制来学习图像和文本之间的对应关系，通过最大化相关图像-文本对的相似度，同时最小化不相关对的相似度。 混合数据预训练：CoCa 利用混合的图像-文本对和图像-标签对进行预训练。这种混合使用使模型能够同时学习丰富的视觉概念和复杂的跨模态关系。 生成分支：CoCa 在传统的对比学习框架上增加了一个生成分支。这个分支可以生成文本描述，从而增强模型的性能，并使模型能够执行图像描述和视觉问答等任务。 学习从零开始的图像编码器：Coca完全从零开始训练，以更深层次地理解视觉内容。 [图片] Coca: Contrastive captioners are image-text foundation models, 2022 CLIP &#43; 其他学习方法 CLIP &#43; Supervised Learning Noisy label &#43; text supervision（噪声标签&#43;文本监督） UniCL ：它提供了一种原则性的方法来同时使用图像-标签和图像-文本数据。它是一种统一的对比学习（Unified Contrastive Learning）框架，旨在优化图像、文本和标签在同一空间内的表征。 [图片] LiT: Locking the image encoder：使用预训练好的image encoder并将它冻结，并添加文本塔实现开放词汇表。使text model学会从image encoder的结果中读取出好的表示。 [图片] MOFI：从带有噪声的实体标注图像中学习图像表示的方法。它结合了监督学习和对比学习，以提高模型在多任务环境中的性能。 [图片] Unified contrastive learning in image-text-label space, CVPR 2022 Lit: Zero-shot transfer with locked-image text tuning, CVPR 2022 MOFI: Learning Image Representations from Noisy Entity Annotated Images, 2023 CLIP &#43; Image-Only (Non-) Contrastive Learning">
<meta name="author" content="Huijie Liu">
<link rel="canonical" href="//localhost:1313/posts/s/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="//localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="//localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="//localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="//localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="//localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="//localhost:1313/posts/s/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="" />
<meta property="og:description" content="原文链接 [图片] Q1: how to learn image representations? Overview [图片] [图片] 改进 CLIP 数据层面：Data scaling up [图片] 模型层面：Model design
image side FLIP（Scaling CLIP training via masking）：是一种改进的训练方法，用于提高 CLIP模型的训练效率。FLIP 的核心思想是在训练过程中随机遮挡图像的部分区域，只对可见的区域进行编码。 Scaling language-image pre-training via masking, CVPR 2023 [图片] language side K-Lite: 将外部知识融入到对比学习预训练中，在 K-Lite 中，实体的维基百科定义（knowledge）可以与原始的图像替代文本（alt-text）一起自然地用于对比预训练。 K-lite: Learning transferable visual models with external knowledge, NeurIPS 2022 [图片] improved interpretability STAIR（Learning Sparse Text and Image Representation in Grounded Tokens）： 将图像和文本映射到高维稀疏嵌入空间； 每个维度的值是一个非负标量，表示与该维度对应的词或标记的权重； 提供了更好的性能和更清晰地识别图像和文本之间的对应关系； STAIR: Learning Sparse Text and Image Representation in Grounded Tokens, 2023 [图片] more modalities ImageBind（One embedding space to bind them all）：将不同模态（在这篇论文中有7种模态）链接到一个共同空间的方法。 一个共同的嵌入空间。 使用冻结的预训练 CLIP。 学习其他模态编码器：对于每个非图像和文本的模态，ImageBind 学习一个编码器，将该模态的数据映射到 CLIP 的嵌入空间中。 [图片] 目标函数：Objective function fine-grained supervision（细粒度监督） FILIP：通过细粒度监督来改进视觉-语言预训练的方法。即单词-图像块（patch）对齐。 双编码器结构：FILIP 仍然使用双编码器结构，即分别对图像和文本进行编码，而不是使用融合编码器。 细粒度监督：FILIP 的核心在于细粒度的监督。它首先计算单词和图像块之间的相似性，然后通过最大池化（max pooling）聚合这个相似性矩阵来计算损失。这种方法允许模型学习更细粒度的图像-文本对齐。 学习单词-图像块对齐：通过这种细粒度的相似性计算和损失函数，FILIP 能够学习单词和图像块之间的对齐关系。这种对齐对于可视化非常有用，因为它提供了图像和文本之间更明确的关联。 FILIP: Fine-grained Interactive Language-Image Pre-Training, ICLR 2022 [图片] adding a generative branch Coca：通过在预训练阶段同时利用图像-文本对和图像-标签对来学习图像和文本的表征。并增加了一个生成分支，以提升模型性能并赋予其新的能力，如图像描述和视觉问答。 对比学习：CoCa 使用对比学习机制来学习图像和文本之间的对应关系，通过最大化相关图像-文本对的相似度，同时最小化不相关对的相似度。 混合数据预训练：CoCa 利用混合的图像-文本对和图像-标签对进行预训练。这种混合使用使模型能够同时学习丰富的视觉概念和复杂的跨模态关系。 生成分支：CoCa 在传统的对比学习框架上增加了一个生成分支。这个分支可以生成文本描述，从而增强模型的性能，并使模型能够执行图像描述和视觉问答等任务。 学习从零开始的图像编码器：Coca完全从零开始训练，以更深层次地理解视觉内容。 [图片] Coca: Contrastive captioners are image-text foundation models, 2022 CLIP &#43; 其他学习方法 CLIP &#43; Supervised Learning Noisy label &#43; text supervision（噪声标签&#43;文本监督） UniCL ：它提供了一种原则性的方法来同时使用图像-标签和图像-文本数据。它是一种统一的对比学习（Unified Contrastive Learning）框架，旨在优化图像、文本和标签在同一空间内的表征。 [图片] LiT: Locking the image encoder：使用预训练好的image encoder并将它冻结，并添加文本塔实现开放词汇表。使text model学会从image encoder的结果中读取出好的表示。 [图片] MOFI：从带有噪声的实体标注图像中学习图像表示的方法。它结合了监督学习和对比学习，以提高模型在多任务环境中的性能。 [图片] Unified contrastive learning in image-text-label space, CVPR 2022 Lit: Zero-shot transfer with locked-image text tuning, CVPR 2022 MOFI: Learning Image Representations from Noisy Entity Annotated Images, 2023 CLIP &#43; Image-Only (Non-) Contrastive Learning" />
<meta property="og:type" content="article" />
<meta property="og:url" content="//localhost:1313/posts/s/" />
<meta property="og:image" content="//localhost:1313/assets/images/profile.png" />
<meta property="article:section" content="posts" />




<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="//localhost:1313/assets/images/profile.png" />
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="原文链接 [图片] Q1: how to learn image representations? Overview [图片] [图片] 改进 CLIP 数据层面：Data scaling up [图片] 模型层面：Model design
image side FLIP（Scaling CLIP training via masking）：是一种改进的训练方法，用于提高 CLIP模型的训练效率。FLIP 的核心思想是在训练过程中随机遮挡图像的部分区域，只对可见的区域进行编码。 Scaling language-image pre-training via masking, CVPR 2023 [图片] language side K-Lite: 将外部知识融入到对比学习预训练中，在 K-Lite 中，实体的维基百科定义（knowledge）可以与原始的图像替代文本（alt-text）一起自然地用于对比预训练。 K-lite: Learning transferable visual models with external knowledge, NeurIPS 2022 [图片] improved interpretability STAIR（Learning Sparse Text and Image Representation in Grounded Tokens）： 将图像和文本映射到高维稀疏嵌入空间； 每个维度的值是一个非负标量，表示与该维度对应的词或标记的权重； 提供了更好的性能和更清晰地识别图像和文本之间的对应关系； STAIR: Learning Sparse Text and Image Representation in Grounded Tokens, 2023 [图片] more modalities ImageBind（One embedding space to bind them all）：将不同模态（在这篇论文中有7种模态）链接到一个共同空间的方法。 一个共同的嵌入空间。 使用冻结的预训练 CLIP。 学习其他模态编码器：对于每个非图像和文本的模态，ImageBind 学习一个编码器，将该模态的数据映射到 CLIP 的嵌入空间中。 [图片] 目标函数：Objective function fine-grained supervision（细粒度监督） FILIP：通过细粒度监督来改进视觉-语言预训练的方法。即单词-图像块（patch）对齐。 双编码器结构：FILIP 仍然使用双编码器结构，即分别对图像和文本进行编码，而不是使用融合编码器。 细粒度监督：FILIP 的核心在于细粒度的监督。它首先计算单词和图像块之间的相似性，然后通过最大池化（max pooling）聚合这个相似性矩阵来计算损失。这种方法允许模型学习更细粒度的图像-文本对齐。 学习单词-图像块对齐：通过这种细粒度的相似性计算和损失函数，FILIP 能够学习单词和图像块之间的对齐关系。这种对齐对于可视化非常有用，因为它提供了图像和文本之间更明确的关联。 FILIP: Fine-grained Interactive Language-Image Pre-Training, ICLR 2022 [图片] adding a generative branch Coca：通过在预训练阶段同时利用图像-文本对和图像-标签对来学习图像和文本的表征。并增加了一个生成分支，以提升模型性能并赋予其新的能力，如图像描述和视觉问答。 对比学习：CoCa 使用对比学习机制来学习图像和文本之间的对应关系，通过最大化相关图像-文本对的相似度，同时最小化不相关对的相似度。 混合数据预训练：CoCa 利用混合的图像-文本对和图像-标签对进行预训练。这种混合使用使模型能够同时学习丰富的视觉概念和复杂的跨模态关系。 生成分支：CoCa 在传统的对比学习框架上增加了一个生成分支。这个分支可以生成文本描述，从而增强模型的性能，并使模型能够执行图像描述和视觉问答等任务。 学习从零开始的图像编码器：Coca完全从零开始训练，以更深层次地理解视觉内容。 [图片] Coca: Contrastive captioners are image-text foundation models, 2022 CLIP &#43; 其他学习方法 CLIP &#43; Supervised Learning Noisy label &#43; text supervision（噪声标签&#43;文本监督） UniCL ：它提供了一种原则性的方法来同时使用图像-标签和图像-文本数据。它是一种统一的对比学习（Unified Contrastive Learning）框架，旨在优化图像、文本和标签在同一空间内的表征。 [图片] LiT: Locking the image encoder：使用预训练好的image encoder并将它冻结，并添加文本塔实现开放词汇表。使text model学会从image encoder的结果中读取出好的表示。 [图片] MOFI：从带有噪声的实体标注图像中学习图像表示的方法。它结合了监督学习和对比学习，以提高模型在多任务环境中的性能。 [图片] Unified contrastive learning in image-text-label space, CVPR 2022 Lit: Zero-shot transfer with locked-image text tuning, CVPR 2022 MOFI: Learning Image Representations from Noisy Entity Annotated Images, 2023 CLIP &#43; Image-Only (Non-) Contrastive Learning"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "//localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "",
      "item": "//localhost:1313/posts/s/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "",
  "name": "",
  "description": "原文链接 [图片] Q1: how to learn image representations? Overview [图片] [图片] 改进 CLIP 数据层面：Data scaling up [图片] 模型层面：Model design\nimage side FLIP（Scaling CLIP training via masking）：是一种改进的训练方法，用于提高 CLIP模型的训练效率。FLIP 的核心思想是在训练过程中随机遮挡图像的部分区域，只对可见的区域进行编码。 Scaling language-image pre-training via masking, CVPR 2023 [图片] language side K-Lite: 将外部知识融入到对比学习预训练中，在 K-Lite 中，实体的维基百科定义（knowledge）可以与原始的图像替代文本（alt-text）一起自然地用于对比预训练。 K-lite: Learning transferable visual models with external knowledge, NeurIPS 2022 [图片] improved interpretability STAIR（Learning Sparse Text and Image Representation in Grounded Tokens）： 将图像和文本映射到高维稀疏嵌入空间； 每个维度的值是一个非负标量，表示与该维度对应的词或标记的权重； 提供了更好的性能和更清晰地识别图像和文本之间的对应关系； STAIR: Learning Sparse Text and Image Representation in Grounded Tokens, 2023 [图片] more modalities ImageBind（One embedding space to bind them all）：将不同模态（在这篇论文中有7种模态）链接到一个共同空间的方法。 一个共同的嵌入空间。 使用冻结的预训练 CLIP。 学习其他模态编码器：对于每个非图像和文本的模态，ImageBind 学习一个编码器，将该模态的数据映射到 CLIP 的嵌入空间中。 [图片] 目标函数：Objective function fine-grained supervision（细粒度监督） FILIP：通过细粒度监督来改进视觉-语言预训练的方法。即单词-图像块（patch）对齐。 双编码器结构：FILIP 仍然使用双编码器结构，即分别对图像和文本进行编码，而不是使用融合编码器。 细粒度监督：FILIP 的核心在于细粒度的监督。它首先计算单词和图像块之间的相似性，然后通过最大池化（max pooling）聚合这个相似性矩阵来计算损失。这种方法允许模型学习更细粒度的图像-文本对齐。 学习单词-图像块对齐：通过这种细粒度的相似性计算和损失函数，FILIP 能够学习单词和图像块之间的对齐关系。这种对齐对于可视化非常有用，因为它提供了图像和文本之间更明确的关联。 FILIP: Fine-grained Interactive Language-Image Pre-Training, ICLR 2022 [图片] adding a generative branch Coca：通过在预训练阶段同时利用图像-文本对和图像-标签对来学习图像和文本的表征。并增加了一个生成分支，以提升模型性能并赋予其新的能力，如图像描述和视觉问答。 对比学习：CoCa 使用对比学习机制来学习图像和文本之间的对应关系，通过最大化相关图像-文本对的相似度，同时最小化不相关对的相似度。 混合数据预训练：CoCa 利用混合的图像-文本对和图像-标签对进行预训练。这种混合使用使模型能够同时学习丰富的视觉概念和复杂的跨模态关系。 生成分支：CoCa 在传统的对比学习框架上增加了一个生成分支。这个分支可以生成文本描述，从而增强模型的性能，并使模型能够执行图像描述和视觉问答等任务。 学习从零开始的图像编码器：Coca完全从零开始训练，以更深层次地理解视觉内容。 [图片] Coca: Contrastive captioners are image-text foundation models, 2022 CLIP + 其他学习方法 CLIP + Supervised Learning Noisy label + text supervision（噪声标签+文本监督） UniCL ：它提供了一种原则性的方法来同时使用图像-标签和图像-文本数据。它是一种统一的对比学习（Unified Contrastive Learning）框架，旨在优化图像、文本和标签在同一空间内的表征。 [图片] LiT: Locking the image encoder：使用预训练好的image encoder并将它冻结，并添加文本塔实现开放词汇表。使text model学会从image encoder的结果中读取出好的表示。 [图片] MOFI：从带有噪声的实体标注图像中学习图像表示的方法。它结合了监督学习和对比学习，以提高模型在多任务环境中的性能。 [图片] Unified contrastive learning in image-text-label space, CVPR 2022 Lit: Zero-shot transfer with locked-image text tuning, CVPR 2022 MOFI: Learning Image Representations from Noisy Entity Annotated Images, 2023 CLIP + Image-Only (Non-) Contrastive Learning",
  "keywords": [
    
  ],
  "articleBody": "原文链接 [图片] Q1: how to learn image representations? Overview [图片] [图片] 改进 CLIP 数据层面：Data scaling up [图片] 模型层面：Model design\nimage side FLIP（Scaling CLIP training via masking）：是一种改进的训练方法，用于提高 CLIP模型的训练效率。FLIP 的核心思想是在训练过程中随机遮挡图像的部分区域，只对可见的区域进行编码。 Scaling language-image pre-training via masking, CVPR 2023 [图片] language side K-Lite: 将外部知识融入到对比学习预训练中，在 K-Lite 中，实体的维基百科定义（knowledge）可以与原始的图像替代文本（alt-text）一起自然地用于对比预训练。 K-lite: Learning transferable visual models with external knowledge, NeurIPS 2022 [图片] improved interpretability STAIR（Learning Sparse Text and Image Representation in Grounded Tokens）： 将图像和文本映射到高维稀疏嵌入空间； 每个维度的值是一个非负标量，表示与该维度对应的词或标记的权重； 提供了更好的性能和更清晰地识别图像和文本之间的对应关系； STAIR: Learning Sparse Text and Image Representation in Grounded Tokens, 2023 [图片] more modalities ImageBind（One embedding space to bind them all）：将不同模态（在这篇论文中有7种模态）链接到一个共同空间的方法。 一个共同的嵌入空间。 使用冻结的预训练 CLIP。 学习其他模态编码器：对于每个非图像和文本的模态，ImageBind 学习一个编码器，将该模态的数据映射到 CLIP 的嵌入空间中。 [图片] 目标函数：Objective function fine-grained supervision（细粒度监督） FILIP：通过细粒度监督来改进视觉-语言预训练的方法。即单词-图像块（patch）对齐。 双编码器结构：FILIP 仍然使用双编码器结构，即分别对图像和文本进行编码，而不是使用融合编码器。 细粒度监督：FILIP 的核心在于细粒度的监督。它首先计算单词和图像块之间的相似性，然后通过最大池化（max pooling）聚合这个相似性矩阵来计算损失。这种方法允许模型学习更细粒度的图像-文本对齐。 学习单词-图像块对齐：通过这种细粒度的相似性计算和损失函数，FILIP 能够学习单词和图像块之间的对齐关系。这种对齐对于可视化非常有用，因为它提供了图像和文本之间更明确的关联。 FILIP: Fine-grained Interactive Language-Image Pre-Training, ICLR 2022 [图片] adding a generative branch Coca：通过在预训练阶段同时利用图像-文本对和图像-标签对来学习图像和文本的表征。并增加了一个生成分支，以提升模型性能并赋予其新的能力，如图像描述和视觉问答。 对比学习：CoCa 使用对比学习机制来学习图像和文本之间的对应关系，通过最大化相关图像-文本对的相似度，同时最小化不相关对的相似度。 混合数据预训练：CoCa 利用混合的图像-文本对和图像-标签对进行预训练。这种混合使用使模型能够同时学习丰富的视觉概念和复杂的跨模态关系。 生成分支：CoCa 在传统的对比学习框架上增加了一个生成分支。这个分支可以生成文本描述，从而增强模型的性能，并使模型能够执行图像描述和视觉问答等任务。 学习从零开始的图像编码器：Coca完全从零开始训练，以更深层次地理解视觉内容。 [图片] Coca: Contrastive captioners are image-text foundation models, 2022 CLIP + 其他学习方法 CLIP + Supervised Learning Noisy label + text supervision（噪声标签+文本监督） UniCL ：它提供了一种原则性的方法来同时使用图像-标签和图像-文本数据。它是一种统一的对比学习（Unified Contrastive Learning）框架，旨在优化图像、文本和标签在同一空间内的表征。 [图片] LiT: Locking the image encoder：使用预训练好的image encoder并将它冻结，并添加文本塔实现开放词汇表。使text model学会从image encoder的结果中读取出好的表示。 [图片] MOFI：从带有噪声的实体标注图像中学习图像表示的方法。它结合了监督学习和对比学习，以提高模型在多任务环境中的性能。 [图片] Unified contrastive learning in image-text-label space, CVPR 2022 Lit: Zero-shot transfer with locked-image text tuning, CVPR 2022 MOFI: Learning Image Representations from Noisy Entity Annotated Images, 2023 CLIP + Image-Only (Non-) Contrastive Learning\nSimCLR：对于给定的图像，应用两种不同的数据增强方法，生成两个变体。这两个变体被视为正样本对，即它们代表同一图像的不同视角，并进行对比训练。 [图片] DeCLIP： 自监督学习： 图像模态：使用类似 SimCLR 的方法，通过双重数据增强和对比学习来学习图像的特征表示。 文本模态：使用掩码语言模型（MLM）的方式来学习文本的特征表示。 多视角监督（Multi-view supervision）：利用来自不同模态（如图像和文本）的信息作为互补的视角，通过对齐这些不同视角的表示来增强学习。 最近邻监督（Nearest-neighbor supervision）：在特征空间中，通过考虑样本的最近邻来引入额外的监督信号，从而促进模型学习更加鲁棒和区分性的特征。 [图片] SLIP：SLIP 的核心思想是将 SimCLR和 CLIP相结合进行模型训练。 [图片] xCLIP： xCLIP = CLIP + nCLIP，是对 CLIP 的一个扩展，它利用图像自监督学习的技术来实现非对比学习，通过正则化和高维投影来确保学到的表示具有足够的区分度和鲁棒性。 引入了锐度（sharpness）和平滑性（smoothness）正则化。锐度正则化鼓励模型学习到尖锐的、区分度高的特征，而平滑性正则化则确保模型不会对数据的微小变化过度敏感。 [图片] A Simple Framework for Contrastive Learning of Visual Representations, ICML 2020 Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm, ICLR 2022 Slip: Self-supervision meets language-image pretraining, ECCV 2022 Non-Contrastive Learning Meets Language-Image Pre-Training, CVPR 2023 Image-Only (Non-) Contrastive Learning + Masked Image Modeling\nBEiT：将自然语言处理中的预训练技术引入到图像领域，并通过视觉标记的概念建立起图像和文本之间的联系。 图像分词器（Image Tokenizer）：在预训练之前，首先使用 VQ-VAE或 GAN等方法学习一个“图像分词器”。将图像分割成一系列离散的视觉标记，类似于 DALL-E 和 Parti 等图像生成模型中使用的方法。 随机遮蔽与预测：在预训练阶段，随机遮蔽图像的一些区域（图像块），然后训练 BEiT 模型去预测这些被遮蔽的视觉标记。 知识蒸馏：这个过程可以理解为图像分词器和 BEiT 编码器之间的知识蒸馏，但后者只能看到图像的一部分。BEiT 编码器通过学习重建遮蔽的视觉标记，从而学习到图像的丰富表示。 [图片] MAE：使用像素值作为目标来训练模型。 大规模遮蔽：在预训练阶段，随机遮蔽图像的大部分区域（例如 75%），只留下一小部分可见区域。 编码器和遮蔽标记：将编码器应用于可见的图像块，以提取特征。在编码器之后引入遮蔽标记。 像素级重建：模型的任务是预测被遮蔽区域的像素值，从而重建整个图像。 适用于目标检测和分割：MAE 预训练对于目标检测和分割等任务特别有帮助，因为它强化了模型对图像局部细节和整体结构的理解。 [图片] MIM很适合模型微调，但不能学习全局图像表示，也不适用大规模数据缩放。 BEiT: BERT Pre-Training of Image Transformers, ICLR 2022 Masked Autoencoders Are Scalable Vision Learners, CVPR 2022 CLIP + Masked Image Modeling （Shallow interaction） CLIP和 MIM是两种不同的自监督学习方法，它们各自专注于不同方面的特征学习。 MVP：使用 CLIP 的图像特征作为 MIM 训练的目标，可以捕获在MIM训练中缺失的语义，这种做法能够结合两种方法的优势，提升模型在捕捉图像语义信息方面的能力。 [图片] EVA：推广了这一方法 [图片] Masked Autoencoding Does Not Help Natural Language Supervision at Scale：MAE对大规模的自然语言监督没有帮助。 [图片] BEiT-3：结合了 BERT和 BEiT的思想，进行多模态数据的遮蔽建模。 多模态变换器（Multiway Transformer）：BEiT-3 使用一个多路变换器来处理图像/文本和图像-文本数据。这种架构允许模型同时处理并整合来自不同模态的信息。 遮蔽数据建模：与单一模态的 MIM 类似，BEiT-3 对图像和文本数据进行遮蔽处理，并训练模型去预测被遮蔽的部分。这种方式促使模型学习到深层次的、跨模态的表示。 共享自注意力层：BEiT-3 设计了共享的自注意力层，这些层可以处理来自不同模态的信息。这种共享机制有助于提高模型的参数效率和泛化能力。 模态专家前馈网络（FFN Modality Experts）：模型包含三个前馈网络（FFN）模态专家，分别专注于处理图像、文本和图像-文本数据。这种设计使得模型能够针对不同模态的数据学习特定的特征。 [图片] MVP: Multimodality-guided Visual Pre-training, ECCV 2022 EVA: Exploring the Limits of Masked Visual Representation Learning at Scale, CVPR 2023 Masked Autoencoding Does Not Help Natural Language Supervision at Scale, CVPR 2023 Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks 主干网络（Backbones） [图片] 总结 三个高层次的原则\n可扩展性（Scaling）：一个好的算法应该简单，但也能够很好地扩展。这意味着算法应该能够处理大规模的数据集和模型，同时保持良好的性能。 对比（Contrasting）：从 SimCLR 到 CLIP，对比学习在自监督学习中扮演着重要角色。通过比较正负样本对，模型学习到区分不同数据点的能力。 遮蔽（Masking）：从 BERT 到 BEiT，遮蔽技术被广泛应用于自监督学习中。通过预测被遮蔽部分的内容，模型能够学习到数据的内在结构和特征。 展望 进一步扩展规模： 数据规模和模型规模的扩展仍是一个挑战。需要探索更有效的方法来处理和学习超大规模的数据集，以及设计更大更强大的模型架构。 新的模型训练范式： 寻找超越 CLIP 和 MIM 的简单且可扩展的算法。这可能涉及到新的学习机制、训练策略或模型架构。 统一的图像-/区域-/像素级预训练： 开发能够在不同粒度级别上对图像进行全面理解的模型。这需要模型能够同时捕捉到全局信息和局部细节。 具有更灵活、可提示（promptable）接口的视觉模型： 将自然语言处理中的概念（如上下文学习、思维链、提示、新兴属性等）引入到计算机视觉中。探索如何使视觉模型能够通过提示进行灵活的交互和学习。 使用更创新的数据训练视觉主干网络： 探索新的数据来源和类型，以解锁模型的新能力，类似于 GPT-4 展示的能力。例如，训练模型阅读整个扫描的论文，然后用几个要点来总结论文内容。 Q2: how to extend vision models with more flexible, promptable interfaces? 计算机视觉领域独有的挑战 模型\n输入格式多种多样：图片，视频，多模态（附带文本/语音） 不同粒度的任务：图片级，区域级，像素级 不同类型的输出：空间输出，文本输出 [图片] 数据 [图片] 如何解决 [图片] Bridge Vision with Language 学习原始视觉信号与丰富语义之间的映射，并可以对各种开放词汇视觉识别任务提供动力 [图片] [图片] GroupViT：通过从头开始学习图像-文本对来学习对语义相似的区域进行分组 使用分组块进行自下而上的分组 自上而下的图像-文本监督以实现视觉语义对齐 [图片] MaskCLIP：从CLIP中提取自由的密集标签 将注意力池更改为新的适应策略 使用CLIP作为教师模型的伪标签掩码 [图片] OpenSeg：弱监督学习，通过强制文本特征和掩码池特征之间的细粒度对齐。 从图像-文本对和局部描述中学习。 使用预训练的掩码建议网络。 [图片] MaskCLIP (UCSD)：使用CLIP作为初始化的COCO全视分割的监督训练 掩码建议网络训练 CLIP模型自适应 [图片] 总结 CLIP作为基础对开放词汇训练有很大帮助 将弱注释与黄金注释结合起来以获得更好的性能 [图片] Unified multi-task modeling 希望开发一个统一的视觉模型，该模型可以在许多视觉任务中表现良好。 [图片] 两种统一化 [图片] Outputs Unification 概述 以一致的格式表示不同输出（一个输出里包含多个任务的结果） [图片] UniTab and Pix2Seqv2 统一词表： 将文本和坐标都进行标记化（tokenization），并放入同一个词表中。 任务前缀： 需要一个任务前缀来确定模型正在处理的任务 [图片] Unified-IO：加入了VQVAE 预训练 VQVAE： 先单独对 VQVAE 进行预训练，让它学习有效地表示和离散化不同类型的视觉信息。 序列到序列的联合训练： 在预训练的 VQVAE 之上，以序列到序列（Sequence-to-Sequence）的方式对整个 Unified-IO 模型进行联合训练。这样做是为了让模型学习如何处理不同模态的输入，并生成符合不同任务要求的序列形式输出。 [图片] 总结 优点： 这个方式可以让一个模型通过自然语言的转换来适应各种视觉任务。 局限性： 仍然需要有任务特定的步骤来把大语言模型的输出转化成可用的结构化数据。 中间过程涉及自然语言，因此模型内部不同任务的交互关系可能会更难以解释。 相比直接输出结构化数据的模型，VisionLLM 在整合不同任务、提升整体性能方面可能存在局限性。 Functionality Unification 概述 知识迁移： 一个模型在特定任务上学习到的能力，可以较容易地迁移到使用相似输出类型的其他任务上。 模型简化： 我们可以设计更具有通用性的模型架构，通过输出层的适配来应对不同的视觉任务。 多任务学习： 既然任务之间存在关联，就有机会设计联合学习方案，让模型同时在多个任务上训练，以相互促进，提升整体性能。 [图片] UniPerceiver-v2 [图片] X-Decoder：允许图像级和像素级的监督信号相互作用。 [图片] LLM-like promptable interface 概述：一个通用的视觉模型应该具有相同的上下文学习能力，在不改变模型参数的情况下将输出与各种用户意图对齐。 上下文记忆 （Context Memory）： 模型需要能存储和处理之前交互轮次中的信息。这可能是通过一个外部记忆模块，或者大规模参数模型本身的记忆能力来实现。 提示构建（Prompt Construction）： 系统能够根据：1) 用户的多轮意图输入； 2) 存储的上下文信息来自动构建模型的提示。提示的内容可能包括任务描述、指示、以及之前的示例交互。 SAM [图片] SEEM: Segment Everything Everywhere all at Once [图片] Q3: how to do image generation? Overview\n4大主题 [图片] 研究总结 [图片] T2I模型总览 [图片] DF原理 [图片] 交叉注意力机制：K和V代表键（Key）和值（Value），它们是从文本流τ(y)投影来的，而Q是从视觉流投影来的，都具有相同的隐藏维度d。因此，Q和K之间的softmax运算产生了一个大小为(hw×d)·(N×d)转置= hw×N的注意力图M。这个交叉注意力图M表示图像和文本之间的细粒度交互，对于文本中的每一个词N，在所有空间位置hw上都有交互。然后注意力图M与V进行点积运算，以产生一个下采样/上采样块的输出。 Spatial Controllable Generation 区域绑定的文本描述：将传统T2I模型中的图像级文本描述扩展到区域绑定的文本描述，使得开放式文本描述能够精确地操作特定的空间区域。 ReCo：其核心思想是扩展文本编码器E的文本词汇，并安排不同的标记以表示绑定的文本输入。该研究通过增加一组位置标记（例如，\u003c687\u003e、\u003c204\u003e、\u003c999\u003e、\u003c833\u003e），这些位置标记与文本标记无缝混合，并作为空间修饰符，指示接下来的文本仅在指定的空间区域操作。 [图片] GLIGEN：采用了一种即插即用的方法，冻结原始T2I模型，并训练额外的门控自注意力层来学习新的定位技能。定位令牌携带两种类型的信息：需要在其中定位的文本词的语义表示和它们的空间配置。然后，通过新增加的门控自注意力层将这些定位令牌添加到预训练的T2I模型中，所有剩余的预训练参数都被冻结。 Reco: Region-controlled text-to-image generation. Gligen: Open-set grounded text-to-image generation 密集空间条件：这一类研究从边界框扩展到以2D数组形式表示的密集空间条件，如分割掩码、边缘图、深度图和关键点等。 ControlNet：向文本提示添加了额外的输入条件。这个额外的条件可以是Canny边缘图、霍夫线、HED边界、素描、人体姿势图、分割掩模、深度图像、法线图或线条图，每个条件都有其独特的模型副本。 [图片] ControlNet：Adding conditional control to text-to-image diffusion models. 推理时指导：前两类工作需要对T2I模型进行微调以理解扩展的空间条件。第三类技术探讨了在不对模型进行微调的情况下实现空间控制的方法。 核心思想与分类器指导（Classifier guidance）类似，即使用判别器损失来引导扩散过程。具体来说，就是在扩散过程的每一步，加入一个额外的项，这个项是由判别器计算出的目标检测损失与期望布局有关的梯度，乘以一个指导强度因子，这样可以在不额外训练模型的情况下实现空间控制。 Text-based Editing 文本到图像编辑（Text-to-image editing）是一种从给定图像和输入的文本描述中合成新图像的技术。可以是之前从文本到图像模型生成的图像，或者是自然图像。目标是保留大部分视觉内容，只修改特定组成部分。 局部图像区域变化：经典的编辑场景之一是改变局部图像区域，例如移除、更换或在某个区域内添加对象。 扩展的空间编辑：语言输入描述了空间区域中期望的外观，语言也可以用作编辑指令告诉机器要做什么，例如“将图像中的对象A更改为对象B”。 综合编辑系统：不是扩展单一的文本到图像（T2I）模型进行编辑，编辑系统集成了不同的专业模块，如分割模型和大型语言模型。 [图片] Text Prompts Following 文本到图像（T2I）模型可能无法遵循文本提示的问题，尤其是当图像描述变得复杂时。例如，某些名词短语可能被遗漏，属性可能应用于错误的对象，生成的图像可能有错误的对象数量、关系、风格等。 推理时操作（Inference-time manipulation）: 在推理阶段，设计各种方法来重新分配视觉潜在表示或图像-文本交叉注意力，以确保文本提示中的所有名词短语都在生成的图像中得到体现。 StructureDiffusion：使用解析树来提取名词短语和文本提示的语言结构，然后强制模型“关注”所有提取的名词短语。这是通过修改交叉注意力机制来实现的，其中 O = M · V，M 是 softmax 交叉注意力图，V 是句子特征。$$O=\\frac1{k+1}\\Sigma_{i=0}^k(M\\cdot V_i)$$，其中 $$V_0$$ 是句子特征 ，$$V_i$$ 是解析树中的短语特征。这种方法确保视觉流在所有识别的名词短语上保持平衡的注意力，从而促进更准确的图像生成。 [图片] Attend-and-Excite：提出了一种正则化损失函数 $$\\begin{aligned}\\ell=\\max_{n=1,…,N_{\\mathrm{sub}}}(1-max\\left.G(M_t^n)\\right)\\end{aligned}$$，用于增强最被忽视的主题tokens的最大注意力。该正则化损失函数使用了一个高斯核 G 来平滑注意力地图，$$N_{sub}$$是主题tokens的数量。然后利用这个损失函数更新在推理时间的潜在表示 zt，更新公式为$$z_t^{\\prime}=z_t-\\alpha\\nabla_{z_t}\\ell$$，$$\\alpha$$是步长大小。 [图片] Training-free structured diffusion guidance for compositional text-to-image synthesis Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. 2. 对齐调优（Alignment tuning）: 学习一个额外的模型学习阶段，通常以图像-文本相似性作为奖励，使得调优后的模型能够更好地遵循文本提示。\nDDPO：加入强化学习 [图片] Training diffusion models with reinforcement learning Concept Customization 直接扩展T2I模型以通过图像输入理解视觉概念。 单一概念定制: 从单一概念定制开始，该过程涉及测试时微调，将视觉概念的多张图片编码成新的Token嵌入。 Textual Inversion：T2I 模型处理不同狗品种的四个图像，随后学习新标记的嵌入，表示为 [V]。这个 [V] 标记可以用作文本标记来表示这个特定的狗。[V] 令牌可以与其他文本描述无缝集成。文本反转通过前缀调整来学习 [V] 令牌嵌入，即冻结所有 T2I 模型的参数并训练 [V] 令牌嵌入以生成输入图像。 Dreambooth：仅调整输入图像可能会导致对特定概念的 T2I 模型过度拟合的风险，为了解决这个问题，本文提出了特定于类的先验保留损失。这种方法的核心是使用预训练的 T2I 模型来生成与目标定制概念相同的类图像。然后，该模型在输入图像（使用 [V] 令牌）和模型生成的图像（没有 [V] 令牌）上联合微调。确保了模型能够在独特的“[V] 狗”和它最初训练的其他一般狗之间进行区分，同时保持其整体的T2I能力。 [图片] [图片] An image is worth one word: Personalizing text-to-image generation using textual inversion Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. 多概念定制: 允许扩展文本到图像模型的Token词汇表，以包括多个概念Tokens，这使得多个概念能够在生成过程中相互作用以及与剩余的视觉场景交互。 Custom Diffusion [图片] Multi-concept customization of text-to-image diffusion. 3. 个性化测试微调的简化: 由于测试时微调要求用户为每个新概念定制T2I模型，这可能会变得相当复杂。为简化使用流程，一些研究探索了无需测试时微调的定制方法，采用统一的微调阶段来扩展T2I模型，使其接受图像条件输入。这些模型将视觉概念的图像作为额外的输入条件，并根据文本描述生成包含视觉概念的图像。\nSuTI：训练单个模型来模拟微调的主题特定专家，并生成以文本和主题输入图像为条件的图像。 [图片] Subject-driven text-to-image generation via apprenticeship learning 展望 Unified image and text inputs（统一图像文本输入） Tuning with alignment-focused loss and rewards（聚焦于对齐的损失和奖励） Closed-loop of multimodal content understanding and generation（多模态内容理解和生成的闭环） Q4: how to train multimodal LLM? Image-to-Text Generative Models 模型架构\n预训练图像编码器与语言模型：这是多模态模型的基础。图像编码器（通常为基于 CNN 或 Transformer 的结构）学习提取图像的视觉特征，而语言模型（如 GPT 系列）负责建模文本序列的内在规律。 连接两个模态的可训练模块：为了融合视觉与语言信息，模型需要有专门的可训练模块。常见的有： Cross-Attentional Mechanisms： 跨模态注意力机制让图像特征与文本相互影响，赋予模型理解图文联系的能力。 Multimodal Fusion Modules： 多模态融合模块负责将来自不同模态的信息聚合，形成统一的表示。 [图片] 训练目标 Cross-Attended Image-to-Text Generation：模型通过跨模态注意力，逐步生成与图像对应的文本描述。这是许多图像描述生成模型的核心目标。 Autoregressive loss on language output：自回归损失用于训练语言模型的生成能力。通过最小化预测的下一个词与真实词之间的差异，使模型学习生成流畅、自然的文本序列。 Large Multimodal Models LLaVA: Large Language-and-Vision Assistant [图片] 总结 [图片] Q5: how to chain vision experts with LLM? LLM + 各种工具 Evolution of Modeling Paradigm [图片] [图片] New Paradigm [图片]\nMM-ReAct: Prompting ChatGPT for Multimodal Reasoning and Action [图片] [图片] ",
  "wordCount" : "836",
  "inLanguage": "en",
  "image": "//localhost:1313/assets/images/profile.png""datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Huijie Liu"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "//localhost:1313/posts/s/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jay Tech",
    "logo": {
      "@type": "ImageObject",
      "url": "//localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="//localhost:1313/" accesskey="h" title="Jay Tech (Alt + H)">Jay Tech</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="//localhost:1313/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/adityatelange/hugo-PaperMod/wiki/" title="WiKi">
                    <span>WiKi</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="//localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="//localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      
    </h1>
    <div class="post-meta">4 min&nbsp;·&nbsp;Huijie Liu

</div>
  </header> 

  <div class="post-content"><p>原文链接
[图片]
Q1: how to learn image representations?
Overview
[图片]
[图片]
改进 CLIP
数据层面：Data scaling up
[图片]
模型层面：Model design</p>
<ul>
<li>image side
<ul>
<li>FLIP（Scaling CLIP training via masking）：是一种改进的训练方法，用于提高 CLIP模型的训练效率。FLIP 的核心思想是在训练过程中随机遮挡图像的部分区域，只对可见的区域进行编码。
Scaling language-image pre-training via masking, CVPR 2023
[图片]</li>
</ul>
</li>
<li>language side
<ul>
<li>K-Lite: 将外部知识融入到对比学习预训练中，在 K-Lite 中，实体的维基百科定义（knowledge）可以与原始的图像替代文本（alt-text）一起自然地用于对比预训练。
K-lite: Learning transferable visual models with external knowledge, NeurIPS 2022
[图片]</li>
</ul>
</li>
<li>improved interpretability
<ul>
<li>STAIR（Learning Sparse Text and Image Representation in Grounded Tokens）：
<ul>
<li>将图像和文本映射到高维稀疏嵌入空间；</li>
<li>每个维度的值是一个非负标量，表示与该维度对应的词或标记的权重；</li>
<li>提供了更好的性能和更清晰地识别图像和文本之间的对应关系；
STAIR: Learning Sparse Text and Image Representation in Grounded Tokens, 2023
[图片]</li>
</ul>
</li>
</ul>
</li>
<li>more modalities
<ul>
<li>ImageBind（One embedding space to bind them all）：将不同模态（在这篇论文中有7种模态）链接到一个共同空间的方法。
<ul>
<li>一个共同的嵌入空间。</li>
<li>使用冻结的预训练 CLIP。</li>
<li>学习其他模态编码器：对于每个非图像和文本的模态，ImageBind 学习一个编码器，将该模态的数据映射到 CLIP 的嵌入空间中。
[图片]
目标函数：Objective function</li>
</ul>
</li>
</ul>
</li>
<li>fine-grained supervision（细粒度监督）
<ul>
<li>FILIP：通过细粒度监督来改进视觉-语言预训练的方法。即单词-图像块（patch）对齐。
<ul>
<li>双编码器结构：FILIP 仍然使用双编码器结构，即分别对图像和文本进行编码，而不是使用融合编码器。</li>
<li>细粒度监督：FILIP 的核心在于细粒度的监督。它首先计算单词和图像块之间的相似性，然后通过最大池化（max pooling）聚合这个相似性矩阵来计算损失。这种方法允许模型学习更细粒度的图像-文本对齐。</li>
<li>学习单词-图像块对齐：通过这种细粒度的相似性计算和损失函数，FILIP 能够学习单词和图像块之间的对齐关系。这种对齐对于可视化非常有用，因为它提供了图像和文本之间更明确的关联。
FILIP: Fine-grained Interactive Language-Image Pre-Training, ICLR 2022
[图片]</li>
</ul>
</li>
</ul>
</li>
<li>adding a generative branch
<ul>
<li>Coca：通过在预训练阶段同时利用图像-文本对和图像-标签对来学习图像和文本的表征。并增加了一个生成分支，以提升模型性能并赋予其新的能力，如图像描述和视觉问答。
<ul>
<li>对比学习：CoCa 使用对比学习机制来学习图像和文本之间的对应关系，通过最大化相关图像-文本对的相似度，同时最小化不相关对的相似度。</li>
<li>混合数据预训练：CoCa 利用混合的图像-文本对和图像-标签对进行预训练。这种混合使用使模型能够同时学习丰富的视觉概念和复杂的跨模态关系。</li>
<li>生成分支：CoCa 在传统的对比学习框架上增加了一个生成分支。这个分支可以生成文本描述，从而增强模型的性能，并使模型能够执行图像描述和视觉问答等任务。</li>
<li>学习从零开始的图像编码器：Coca完全从零开始训练，以更深层次地理解视觉内容。
[图片]
Coca: Contrastive captioners are image-text foundation models, 2022
CLIP + 其他学习方法
CLIP + Supervised Learning
Noisy label + text supervision（噪声标签+文本监督）</li>
</ul>
</li>
</ul>
</li>
<li>UniCL ：它提供了一种原则性的方法来同时使用图像-标签和图像-文本数据。它是一种统一的对比学习（Unified Contrastive Learning）框架，旨在优化图像、文本和标签在同一空间内的表征。
[图片]</li>
<li>LiT: Locking the image encoder：使用预训练好的image encoder并将它冻结，并添加文本塔实现开放词汇表。使text model学会从image encoder的结果中读取出好的表示。
[图片]</li>
<li>MOFI：从带有噪声的实体标注图像中学习图像表示的方法。它结合了监督学习和对比学习，以提高模型在多任务环境中的性能。
[图片]</li>
</ul>
<p>Unified contrastive learning in image-text-label space, CVPR 2022
Lit: Zero-shot transfer with locked-image text tuning, CVPR 2022
MOFI: Learning Image Representations from Noisy Entity Annotated Images, 2023
CLIP + Image-Only (Non-) Contrastive Learning</p>
<ul>
<li>SimCLR：对于给定的图像，应用两种不同的数据增强方法，生成两个变体。这两个变体被视为正样本对，即它们代表同一图像的不同视角，并进行对比训练。
[图片]</li>
<li>DeCLIP：
<ol>
<li>自监督学习：</li>
</ol>
<ul>
<li>图像模态：使用类似 SimCLR 的方法，通过双重数据增强和对比学习来学习图像的特征表示。</li>
<li>文本模态：使用掩码语言模型（MLM）的方式来学习文本的特征表示。</li>
</ul>
<ol start="2">
<li>多视角监督（Multi-view supervision）：利用来自不同模态（如图像和文本）的信息作为互补的视角，通过对齐这些不同视角的表示来增强学习。</li>
<li>最近邻监督（Nearest-neighbor supervision）：在特征空间中，通过考虑样本的最近邻来引入额外的监督信号，从而促进模型学习更加鲁棒和区分性的特征。
[图片]</li>
</ol>
</li>
<li>SLIP：SLIP 的核心思想是将 SimCLR和 CLIP相结合进行模型训练。
[图片]</li>
<li>xCLIP： xCLIP = CLIP + nCLIP，是对 CLIP 的一个扩展，它利用图像自监督学习的技术来实现非对比学习，通过正则化和高维投影来确保学到的表示具有足够的区分度和鲁棒性。 引入了锐度（sharpness）和平滑性（smoothness）正则化。锐度正则化鼓励模型学习到尖锐的、区分度高的特征，而平滑性正则化则确保模型不会对数据的微小变化过度敏感。
[图片]</li>
</ul>
<p>A Simple Framework for Contrastive Learning of Visual Representations, ICML 2020
Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm, ICLR 2022
Slip: Self-supervision meets language-image pretraining, ECCV 2022
Non-Contrastive Learning Meets Language-Image Pre-Training, CVPR 2023
Image-Only (Non-) Contrastive Learning + Masked Image Modeling</p>
<ul>
<li>BEiT：将自然语言处理中的预训练技术引入到图像领域，并通过视觉标记的概念建立起图像和文本之间的联系。
<ul>
<li>图像分词器（Image Tokenizer）：在预训练之前，首先使用 VQ-VAE或 GAN等方法学习一个“图像分词器”。将图像分割成一系列离散的视觉标记，类似于 DALL-E 和 Parti 等图像生成模型中使用的方法。</li>
<li>随机遮蔽与预测：在预训练阶段，随机遮蔽图像的一些区域（图像块），然后训练 BEiT 模型去预测这些被遮蔽的视觉标记。</li>
<li>知识蒸馏：这个过程可以理解为图像分词器和 BEiT 编码器之间的知识蒸馏，但后者只能看到图像的一部分。BEiT 编码器通过学习重建遮蔽的视觉标记，从而学习到图像的丰富表示。
[图片]</li>
</ul>
</li>
<li>MAE：使用像素值作为目标来训练模型。
<ul>
<li>大规模遮蔽：在预训练阶段，随机遮蔽图像的大部分区域（例如 75%），只留下一小部分可见区域。</li>
<li>编码器和遮蔽标记：将编码器应用于可见的图像块，以提取特征。在编码器之后引入遮蔽标记。</li>
<li>像素级重建：模型的任务是预测被遮蔽区域的像素值，从而重建整个图像。</li>
<li>适用于目标检测和分割：MAE 预训练对于目标检测和分割等任务特别有帮助，因为它强化了模型对图像局部细节和整体结构的理解。
[图片]
MIM很适合模型微调，但不能学习全局图像表示，也不适用大规模数据缩放。
BEiT: BERT Pre-Training of Image Transformers, ICLR 2022
Masked Autoencoders Are Scalable Vision Learners, CVPR 2022
CLIP + Masked Image Modeling （Shallow interaction）
CLIP和 MIM是两种不同的自监督学习方法，它们各自专注于不同方面的特征学习。</li>
</ul>
</li>
<li>MVP：使用 CLIP 的图像特征作为 MIM 训练的目标，可以捕获在MIM训练中缺失的语义，这种做法能够结合两种方法的优势，提升模型在捕捉图像语义信息方面的能力。
[图片]</li>
<li>EVA：推广了这一方法
[图片]</li>
<li>Masked Autoencoding Does Not Help Natural Language Supervision at Scale：MAE对大规模的自然语言监督没有帮助。
[图片]</li>
<li>BEiT-3：结合了 BERT和 BEiT的思想，进行多模态数据的遮蔽建模。
<ul>
<li>多模态变换器（Multiway Transformer）：BEiT-3 使用一个多路变换器来处理图像/文本和图像-文本数据。这种架构允许模型同时处理并整合来自不同模态的信息。</li>
<li>遮蔽数据建模：与单一模态的 MIM 类似，BEiT-3 对图像和文本数据进行遮蔽处理，并训练模型去预测被遮蔽的部分。这种方式促使模型学习到深层次的、跨模态的表示。</li>
<li>共享自注意力层：BEiT-3 设计了共享的自注意力层，这些层可以处理来自不同模态的信息。这种共享机制有助于提高模型的参数效率和泛化能力。</li>
<li>模态专家前馈网络（FFN Modality Experts）：模型包含三个前馈网络（FFN）模态专家，分别专注于处理图像、文本和图像-文本数据。这种设计使得模型能够针对不同模态的数据学习特定的特征。
[图片]</li>
</ul>
</li>
</ul>
<p>MVP: Multimodality-guided Visual Pre-training, ECCV 2022
EVA: Exploring the Limits of Masked Visual Representation Learning at Scale, CVPR 2023
Masked Autoencoding Does Not Help Natural Language Supervision at Scale, CVPR 2023
Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks
主干网络（Backbones）
[图片]
总结
三个高层次的原则</p>
<ol>
<li>可扩展性（Scaling）：一个好的算法应该简单，但也能够很好地扩展。这意味着算法应该能够处理大规模的数据集和模型，同时保持良好的性能。</li>
<li>对比（Contrasting）：从 SimCLR 到 CLIP，对比学习在自监督学习中扮演着重要角色。通过比较正负样本对，模型学习到区分不同数据点的能力。</li>
<li>遮蔽（Masking）：从 BERT 到 BEiT，遮蔽技术被广泛应用于自监督学习中。通过预测被遮蔽部分的内容，模型能够学习到数据的内在结构和特征。
展望</li>
<li>进一步扩展规模：
数据规模和模型规模的扩展仍是一个挑战。需要探索更有效的方法来处理和学习超大规模的数据集，以及设计更大更强大的模型架构。</li>
<li>新的模型训练范式：
寻找超越 CLIP 和 MIM 的简单且可扩展的算法。这可能涉及到新的学习机制、训练策略或模型架构。</li>
<li>统一的图像-/区域-/像素级预训练：
开发能够在不同粒度级别上对图像进行全面理解的模型。这需要模型能够同时捕捉到全局信息和局部细节。</li>
<li>具有更灵活、可提示（promptable）接口的视觉模型：
将自然语言处理中的概念（如上下文学习、思维链、提示、新兴属性等）引入到计算机视觉中。探索如何使视觉模型能够通过提示进行灵活的交互和学习。</li>
<li>使用更创新的数据训练视觉主干网络：
探索新的数据来源和类型，以解锁模型的新能力，类似于 GPT-4 展示的能力。例如，训练模型阅读整个扫描的论文，然后用几个要点来总结论文内容。</li>
</ol>
<hr>
<p>Q2: how to extend vision models with more flexible, promptable interfaces?
计算机视觉领域独有的挑战
模型</p>
<ol>
<li>输入格式多种多样：图片，视频，多模态（附带文本/语音）</li>
<li>不同粒度的任务：图片级，区域级，像素级</li>
<li>不同类型的输出：空间输出，文本输出
[图片]
数据
[图片]
如何解决
[图片]
Bridge Vision with Language
学习原始视觉信号与丰富语义之间的映射，并可以对各种开放词汇视觉识别任务提供动力
[图片]
[图片]</li>
</ol>
<ul>
<li>GroupViT：通过从头开始学习图像-文本对来学习对语义相似的区域进行分组
<ul>
<li>使用分组块进行自下而上的分组</li>
<li>自上而下的图像-文本监督以实现视觉语义对齐
[图片]</li>
</ul>
</li>
<li>MaskCLIP：从CLIP中提取自由的密集标签
<ul>
<li>将注意力池更改为新的适应策略</li>
<li>使用CLIP作为教师模型的伪标签掩码
[图片]</li>
</ul>
</li>
<li>OpenSeg：弱监督学习，通过强制文本特征和掩码池特征之间的细粒度对齐。
<ul>
<li>从图像-文本对和局部描述中学习。</li>
<li>使用预训练的掩码建议网络。
[图片]</li>
</ul>
</li>
<li>MaskCLIP (UCSD)：使用CLIP作为初始化的COCO全视分割的监督训练
<ul>
<li>掩码建议网络训练</li>
<li>CLIP模型自适应
[图片]</li>
</ul>
</li>
<li>总结
<ul>
<li>CLIP作为基础对开放词汇训练有很大帮助</li>
<li>将弱注释与黄金注释结合起来以获得更好的性能
[图片]
Unified multi-task modeling
希望开发一个统一的视觉模型，该模型可以在许多视觉任务中表现良好。
[图片]
两种统一化
[图片]</li>
</ul>
</li>
<li>Outputs Unification
<ul>
<li>概述
<ul>
<li>以一致的格式表示不同输出（一个输出里包含多个任务的结果）
[图片]</li>
</ul>
</li>
<li>UniTab and Pix2Seqv2</li>
</ul>
<ol>
<li>统一词表： 将文本和坐标都进行标记化（tokenization），并放入同一个词表中。</li>
<li>任务前缀： 需要一个任务前缀来确定模型正在处理的任务
[图片]</li>
</ol>
<ul>
<li>Unified-IO：加入了VQVAE
<ol>
<li>预训练 VQVAE： 先单独对 VQVAE 进行预训练，让它学习有效地表示和离散化不同类型的视觉信息。</li>
<li>序列到序列的联合训练： 在预训练的 VQVAE 之上，以序列到序列（Sequence-to-Sequence）的方式对整个 Unified-IO 模型进行联合训练。这样做是为了让模型学习如何处理不同模态的输入，并生成符合不同任务要求的序列形式输出。
[图片]</li>
</ol>
</li>
<li>总结
<ul>
<li>优点： 这个方式可以让一个模型通过自然语言的转换来适应各种视觉任务。</li>
<li>局限性：
<ul>
<li>仍然需要有任务特定的步骤来把大语言模型的输出转化成可用的结构化数据。</li>
<li>中间过程涉及自然语言，因此模型内部不同任务的交互关系可能会更难以解释。</li>
<li>相比直接输出结构化数据的模型，VisionLLM 在整合不同任务、提升整体性能方面可能存在局限性。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Functionality Unification
<ul>
<li>概述
<ul>
<li>知识迁移： 一个模型在特定任务上学习到的能力，可以较容易地迁移到使用相似输出类型的其他任务上。</li>
<li>模型简化： 我们可以设计更具有通用性的模型架构，通过输出层的适配来应对不同的视觉任务。</li>
<li>多任务学习： 既然任务之间存在关联，就有机会设计联合学习方案，让模型同时在多个任务上训练，以相互促进，提升整体性能。
[图片]</li>
</ul>
</li>
<li>UniPerceiver-v2
[图片]</li>
<li>X-Decoder：允许图像级和像素级的监督信号相互作用。
[图片]
LLM-like promptable interface</li>
</ul>
</li>
<li>概述：一个通用的视觉模型应该具有相同的上下文学习能力，在不改变模型参数的情况下将输出与各种用户意图对齐。
<ul>
<li>上下文记忆 （Context Memory）： 模型需要能存储和处理之前交互轮次中的信息。这可能是通过一个外部记忆模块，或者大规模参数模型本身的记忆能力来实现。</li>
<li>提示构建（Prompt Construction）： 系统能够根据：1) 用户的多轮意图输入； 2) 存储的上下文信息来自动构建模型的提示。提示的内容可能包括任务描述、指示、以及之前的示例交互。</li>
</ul>
</li>
<li>SAM
[图片]</li>
<li>SEEM: Segment Everything Everywhere all at Once
[图片]</li>
</ul>
<hr>
<p>Q3: how to do image generation?
Overview</p>
<ul>
<li>4大主题
[图片]</li>
<li>研究总结
[图片]</li>
<li>T2I模型总览
[图片]</li>
<li>DF原理
[图片]
交叉注意力机制：K和V代表键（Key）和值（Value），它们是从文本流τ(y)投影来的，而Q是从视觉流投影来的，都具有相同的隐藏维度d。因此，Q和K之间的softmax运算产生了一个大小为(hw×d)·(N×d)转置= hw×N的注意力图M。这个交叉注意力图M表示图像和文本之间的细粒度交互，对于文本中的每一个词N，在所有空间位置hw上都有交互。然后注意力图M与V进行点积运算，以产生一个下采样/上采样块的输出。
Spatial Controllable Generation</li>
</ul>
<ol>
<li>区域绑定的文本描述：将传统T2I模型中的图像级文本描述扩展到区域绑定的文本描述，使得开放式文本描述能够精确地操作特定的空间区域。</li>
</ol>
<ul>
<li>ReCo：其核心思想是扩展文本编码器E的文本词汇，并安排不同的标记以表示绑定的文本输入。该研究通过增加一组位置标记（例如，&lt;687&gt;、&lt;204&gt;、&lt;999&gt;、&lt;833&gt;），这些位置标记与文本标记无缝混合，并作为空间修饰符，指示接下来的文本仅在指定的空间区域操作。
[图片]</li>
<li>GLIGEN：采用了一种即插即用的方法，冻结原始T2I模型，并训练额外的门控自注意力层来学习新的定位技能。定位令牌携带两种类型的信息：需要在其中定位的文本词的语义表示和它们的空间配置。然后，通过新增加的门控自注意力层将这些定位令牌添加到预训练的T2I模型中，所有剩余的预训练参数都被冻结。
Reco: Region-controlled text-to-image generation.
Gligen: Open-set grounded text-to-image generation</li>
</ul>
<ol start="2">
<li>密集空间条件：这一类研究从边界框扩展到以2D数组形式表示的密集空间条件，如分割掩码、边缘图、深度图和关键点等。</li>
</ol>
<ul>
<li>ControlNet：向文本提示添加了额外的输入条件。这个额外的条件可以是Canny边缘图、霍夫线、HED边界、素描、人体姿势图、分割掩模、深度图像、法线图或线条图，每个条件都有其独特的模型副本。
[图片]
ControlNet：Adding conditional control to text-to-image diffusion models.</li>
</ul>
<ol start="3">
<li>推理时指导：前两类工作需要对T2I模型进行微调以理解扩展的空间条件。第三类技术探讨了在不对模型进行微调的情况下实现空间控制的方法。
核心思想与分类器指导（Classifier guidance）类似，即使用判别器损失来引导扩散过程。具体来说，就是在扩散过程的每一步，加入一个额外的项，这个项是由判别器计算出的目标检测损失与期望布局有关的梯度，乘以一个指导强度因子，这样可以在不额外训练模型的情况下实现空间控制。
Text-based Editing
文本到图像编辑（Text-to-image editing）是一种从给定图像和输入的文本描述中合成新图像的技术。可以是之前从文本到图像模型生成的图像，或者是自然图像。目标是保留大部分视觉内容，只修改特定组成部分。</li>
<li>局部图像区域变化：经典的编辑场景之一是改变局部图像区域，例如移除、更换或在某个区域内添加对象。</li>
<li>扩展的空间编辑：语言输入描述了空间区域中期望的外观，语言也可以用作编辑指令告诉机器要做什么，例如“将图像中的对象A更改为对象B”。</li>
<li>综合编辑系统：不是扩展单一的文本到图像（T2I）模型进行编辑，编辑系统集成了不同的专业模块，如分割模型和大型语言模型。
[图片]
Text Prompts Following
文本到图像（T2I）模型可能无法遵循文本提示的问题，尤其是当图像描述变得复杂时。例如，某些名词短语可能被遗漏，属性可能应用于错误的对象，生成的图像可能有错误的对象数量、关系、风格等。</li>
<li>推理时操作（Inference-time manipulation）:
在推理阶段，设计各种方法来重新分配视觉潜在表示或图像-文本交叉注意力，以确保文本提示中的所有名词短语都在生成的图像中得到体现。</li>
</ol>
<ul>
<li>StructureDiffusion：使用解析树来提取名词短语和文本提示的语言结构，然后强制模型“关注”所有提取的名词短语。这是通过修改交叉注意力机制来实现的，其中 O = M · V，M 是 softmax 交叉注意力图，V 是句子特征。$$O=\frac1{k+1}\Sigma_{i=0}^k(M\cdot V_i)$$，其中 $$V_0$$ 是句子特征 ，$$V_i$$ 是解析树中的短语特征。这种方法确保视觉流在所有识别的名词短语上保持平衡的注意力，从而促进更准确的图像生成。
[图片]</li>
<li>Attend-and-Excite：提出了一种正则化损失函数 $$\begin{aligned}\ell=\max_{n=1,&hellip;,N_{\mathrm{sub}}}(1-max\left.G(M_t^n)\right)\end{aligned}$$，用于增强最被忽视的主题tokens的最大注意力。该正则化损失函数使用了一个高斯核 G 来平滑注意力地图，$$N_{sub}$$是主题tokens的数量。然后利用这个损失函数更新在推理时间的潜在表示 zt，更新公式为$$z_t^{\prime}=z_t-\alpha\nabla_{z_t}\ell$$，$$\alpha$$是步长大小。
[图片]</li>
</ul>
<p>Training-free structured diffusion guidance for compositional text-to-image synthesis
Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models.
2. 对齐调优（Alignment tuning）:
学习一个额外的模型学习阶段，通常以图像-文本相似性作为奖励，使得调优后的模型能够更好地遵循文本提示。</p>
<ul>
<li>DDPO：加入强化学习
[图片]
Training diffusion models with reinforcement learning
Concept Customization
直接扩展T2I模型以通过图像输入理解视觉概念。</li>
</ul>
<ol>
<li>单一概念定制: 从单一概念定制开始，该过程涉及测试时微调，将视觉概念的多张图片编码成新的Token嵌入。</li>
</ol>
<ul>
<li>Textual Inversion：T2I 模型处理不同狗品种的四个图像，随后学习新标记的嵌入，表示为 [V]。这个 [V] 标记可以用作文本标记来表示这个特定的狗。[V] 令牌可以与其他文本描述无缝集成。文本反转通过前缀调整来学习 [V] 令牌嵌入，即冻结所有 T2I 模型的参数并训练 [V] 令牌嵌入以生成输入图像。</li>
<li>Dreambooth：仅调整输入图像可能会导致对特定概念的 T2I 模型过度拟合的风险，为了解决这个问题，本文提出了特定于类的先验保留损失。这种方法的核心是使用预训练的 T2I 模型来生成与目标定制概念相同的类图像。然后，该模型在输入图像（使用 [V] 令牌）和模型生成的图像（没有 [V] 令牌）上联合微调。确保了模型能够在独特的“[V] 狗”和它最初训练的其他一般狗之间进行区分，同时保持其整体的T2I能力。
[图片]
[图片]
An image is worth one word: Personalizing text-to-image generation using textual inversion
Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.</li>
</ul>
<ol start="2">
<li>多概念定制: 允许扩展文本到图像模型的Token词汇表，以包括多个概念Tokens，这使得多个概念能够在生成过程中相互作用以及与剩余的视觉场景交互。</li>
</ol>
<ul>
<li>Custom Diffusion
[图片]</li>
</ul>
<p>Multi-concept customization of text-to-image diffusion.
3. 个性化测试微调的简化: 由于测试时微调要求用户为每个新概念定制T2I模型，这可能会变得相当复杂。为简化使用流程，一些研究探索了无需测试时微调的定制方法，采用统一的微调阶段来扩展T2I模型，使其接受图像条件输入。这些模型将视觉概念的图像作为额外的输入条件，并根据文本描述生成包含视觉概念的图像。</p>
<ul>
<li>SuTI：训练单个模型来模拟微调的主题特定专家，并生成以文本和主题输入图像为条件的图像。
[图片]
Subject-driven text-to-image generation via apprenticeship learning
展望</li>
<li>Unified image and text inputs（统一图像文本输入）</li>
<li>Tuning with alignment-focused loss and rewards（聚焦于对齐的损失和奖励）</li>
<li>Closed-loop of multimodal content understanding and generation（多模态内容理解和生成的闭环）</li>
</ul>
<hr>
<p>Q4: how to train multimodal LLM?
Image-to-Text Generative Models
模型架构</p>
<ul>
<li>预训练图像编码器与语言模型：这是多模态模型的基础。图像编码器（通常为基于 CNN 或 Transformer 的结构）学习提取图像的视觉特征，而语言模型（如 GPT 系列）负责建模文本序列的内在规律。</li>
<li>连接两个模态的可训练模块：为了融合视觉与语言信息，模型需要有专门的可训练模块。常见的有：
<ul>
<li>Cross-Attentional Mechanisms： 跨模态注意力机制让图像特征与文本相互影响，赋予模型理解图文联系的能力。</li>
<li>Multimodal Fusion Modules： 多模态融合模块负责将来自不同模态的信息聚合，形成统一的表示。
[图片]
训练目标</li>
</ul>
</li>
<li>Cross-Attended Image-to-Text Generation：模型通过跨模态注意力，逐步生成与图像对应的文本描述。这是许多图像描述生成模型的核心目标。</li>
<li>Autoregressive loss on language output：自回归损失用于训练语言模型的生成能力。通过最小化预测的下一个词与真实词之间的差异，使模型学习生成流畅、自然的文本序列。
Large Multimodal Models</li>
<li>LLaVA: Large Language-and-Vision Assistant
[图片]
总结
[图片]</li>
</ul>
<hr>
<p>Q5: how to chain vision experts with LLM?
LLM + 各种工具
Evolution of Modeling Paradigm
[图片]
[图片]
New Paradigm
[图片]</p>
<ul>
<li>MM-ReAct: Prompting ChatGPT for Multimodal Reasoning and Action
[图片]
[图片]</li>
</ul>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="//localhost:1313/posts/deep-learning/">
    <span class="title">« Prev</span>
    <br>
    <span>Deep Learning</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="//localhost:1313/">Jay Tech</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
