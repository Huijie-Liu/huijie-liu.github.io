[{"content":"3DGS 基本思想 3D高斯分布可以通过它们的各向异性协方差矩阵、位置和透明度等参数来有效地表示复杂场景。由于这些参数是通过机器学习方法进行训练的，渲染阶段无需进行大量处理。因此，它可以利用基于瓦片的光栅化器实现快速渲染，从而在性能上有显著的提升。 创新点 Point-Based Rendering：点基渲染直接将三维空间中的点渲染为图像。 Tiled Rasterization：分块光栅化的基本思想是将屏幕划分为多个小块（Tiles），然后在每个小块内进行相关计算和处理（可微分）。这种方法能够显著减少内存流量，从而提高渲染效率。 Spherical Harmonics：球谐函数是一种在球面上表示函数的方法，特别适用于描述球形表面的光照和阴影效果。 基本流程 收集数据\n图像\n视频-\u0026gt;ffmpeg截取视频帧\nffmpeg -i \u0026lt;VIDEO_PATH\u0026gt; -qscale:v 1 -qmin 1 -vf fps=2 %04d.jpg 输出如下\n📦 $FOLDER_PATH ┣ 📂 input ┃ ┣ 📜 000000.jpg ┃ ┣ 📜 000001.jpg ┃ ┣ 📜 ... 获取相机位姿\nCOLMAP：开源Structure-from-Motion (SfM) 软件，输入images，输出相机位姿\n原论文使用的是自带的convert.py，自动调用COLMAP并转换成需要的格式\n桌面软件：RealityCapture, Metashape\n移动app：Polycam, Record3D（利用了雷达）\n输出如下：\n📦 $FOLDER_PATH ┣ 📂 (input) ┣ 📂 (distorted) ┣ 📂 images ┣ 📂 sparse ┃ ┣ 📂 0 ┃ ┃ ┣ 📜 points3D.bin ┃ ┃ ┣ 📜 images.bin ┃ ┃ ┗ 📜 cameras.bin 训练\n整个训练过程（30,000步）大约需要30-40分钟，在完成7,000步之后会保存一个中间模型。\n输出如下：\n📦 $FOLDER_PATH ┣ 📂 images ┣ 📂 sparse ┣ 📂 output ┃ ┣ 📜 cameras.json ┃ ┣ 📜 cfg_args ┃ ┗ 📜 input.ply ┃ ┣ 📂 point_cloud ┃ ┃ ┣ 📂 iteration_7000 ┃ ┃ ┃ ┗ 📜 point_cloud.ply ┃ ┃ ┣ 📂 iteration_30000 ┃ ┃ ┃ ┗ 📜 point_cloud.ply 可视化\n（官方）在Windows上安装预编译的SIBR viewer （官方）在Ubuntu 上构建SIBR viewer （第三方）SuperSplat，Three.js 原理详解 光栅化：概述 对比NeRF（辐射场）\nNeRF\n$$\\begin{aligned}\u0026C(p)=\\\\\u0026=\\sum_{i=1}^Nc_i(1-\\exp(-\\sigma_i\\delta_i))T_i=\\\\\u0026=\\sum_{i=1}^Nc_i(1-\\exp(-\\sigma_i\\delta_i))\\exp(-\\sum_{j=1}^{i-1}\\sigma_j\\delta_j)=\u0026(1)\\\\\u0026=\\sum_{i=1}^Nc_i\\underbrace{(1-\\exp(-\\sigma_i\\delta_i))}_{\\alpha_i}\\prod_{j=1}^{i-1}\\underbrace{\\exp(-\\sigma_j\\delta_j)}_{1-\\alpha_j}=\\\\\u0026=\\sum_{i=1}^Nc_i\\alpha_i\\underbrace{\\prod_{j=1}^{i-1}(1-\\alpha_j)}_{transmittance}\u0026(2)\\end{aligned}$$ 3DGS\n$$C(p)=\\sum_{i\\in N}c_if_i^{2D}(p)\\underbrace{\\prod_{j=1}^{i-1}(1-f_j^{2D}(p))}_{transmittance}\\quad(3)$$ 公式（3）描述了如何在一个像素中获得颜色值。要渲染整个图像，仍然需要遍历所有的H×W射线，就像在 NeRF 中一样。不过，这个过程更加轻量化：\n预处理排序阶段： 每帧只需在GPU上进行一次预处理排序，使用定制的可微分CUDA 内核实现。这一步骤可以显著加速渲染过程，因为它利用了 GPU 的并行计算能力。 在这一步骤中，所有的三维点根据它们在二维图像平面上的投影位置进行排序，以便快速查找和混合。 预先投影到2D： 对于给定的相机，可以提前将每个三维点的f（p）投影到二维。在遍历像素之前完成这个步骤，这样当高斯函数混合到附近的几个像素时，不需要一遍又一遍地重新投影。 例如，假设有一个三维点（X，Y，2），在给定相机内参和外参矩阵的情况下，可以提前计算出该点在图像平面上的投影位置（2,g）。 直接混合2D高斯： 不需要为每个像素、每条射线、每个三维点运行多层感知机（MLP）模型推断。相反，二维高斯函数可以直接混合到图像上。 这意味着在渲染过程中，计算量大大减少，因为不再需要运行复杂的神经网络推断。 固定的三维点集： 没有模糊性，不需要沿射线选择三维点进行评估，也不需要选择射线采样策略。每个像素的射线重叠的三维点集（公式（3）中的N）在优化后是离散且固定的。 例如，假设某个像素的射线经过了几个三维点，这些点在优化之后是固定的，因此可以直接使用这些点进行渲染，而不需要每次重新采样。 光栅化：实现细节（前向传播） 3D高斯体通过投影矩阵转换到二维相机平面上，获得其投影位置和范围，接着根据深度进行排序，并且从前到后按照不透明度和颜色进行alpha混合，最终组合生成输出图像。\n一个三维高斯分布由以下参数化：\n均值 $\\mu \\in \\mathbb{R}^3$：三维空间中的位置。 协方差 $\\Sigma \\in \\mathbb{R}^{3 \\times 3}$：描述高斯分布的形状和方向。 颜色 $c \\in \\mathbb{R}^3$：颜色向量，通常表示为 RGB 值。 不透明度 $o \\in \\mathbb{R}$：描述高斯分布的透明度。 高斯分布的投影（3D-\u0026gt;2D） 世界坐标系转-\u0026gt;相机坐标系\n渲染相机由其外参 $T_{cw} $描述，它将点从世界坐标系转换到相机坐标系，以及其内参（焦距 $f_x, f_y $和相机平面主点 $(c_x, c_y)$）。我们使用投影矩阵 P 将相机空间的转换到标准化剪辑空间。\n$$T_{cw} = \\begin{bmatrix} R_{cw} \u0026 t_{cw} \\\\ 0 \u0026 1 \\end{bmatrix} \\in SE(3), \\quad P = \\begin{bmatrix} \\frac{2f_x}{w} \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 \\frac{2f_y}{h} \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 \\frac{f+n}{f-n} \u0026 \\frac{-2fn}{f-n} \\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{bmatrix}$$ 其中 w, h 是输出图像的宽度和高度，n, f 是近剪裁平面和远剪裁平面。我们通过标准透视投影将三维均值 \\mu 投影到像素空间。我们将均值 \\mu 转换为相机坐标系中的 $t \\in \\mathbb{R}^4$，在标准化设备坐标中的 $t' \\in \\mathbb{R}^4$，以及在像素坐标中的 $\\mu' \\in \\mathbb{R}^2$。\n$$t = T_{cw} \\begin{bmatrix} \\mu \\\\ 1 \\end{bmatrix}^T, \\quad t' = Pt, \\quad \\mu' = \\left[ \\begin{array}{c} (w \\cdot \\frac{t'_x}{t'_w} + 1)/2 + c_x \\\\ (h \\cdot \\frac{t'_y}{t'_w} + 1)/2 + c_y \\end{array} \\right]$$ 其中 w 和 h 分别是输出图像的宽度和高度。\n三维高斯-\u0026gt;二维高斯\n透视投影一个三维高斯分布并不会产生二维高斯分布。我们使用一阶泰勒展开近似在相机坐标系中的 t 处的投影。具体来说，我们计算仿射变换矩阵 $J \\in \\mathbb{R}^{2 \\times 3} $如下：\n$$J = \\begin{bmatrix} \\frac{f_x}{t_z} \u0026 0 \u0026 -\\frac{f_x \\cdot t_x}{t_z^2} \\\\ 0 \u0026 \\frac{f_y}{t_z} \u0026 -\\frac{f_y \\cdot t_y}{t_z^2} \\end{bmatrix}$$ 二维协方差矩阵 $\\Sigma' \\in \\mathbb{R}^{2 \\times 2}$ 由下式给出：\n$\\Sigma' = JR_{cw} \\Sigma R_{cw}^T J^T$\n最后，我们用尺度 $s \\in \\mathbb{R}^3$ 和旋转四元数 $q \\in \\mathbb{R}^4$ 来参数化三维协方差$ \\Sigma$ 。我们首先将四元数 $q = (x, y, z, w) $转换为旋转矩阵：\n$$R = \\begin{bmatrix} 1 - 2(y^2 + z^2) \u0026 2(xy - wz) \u0026 2(xz + wy) \\\\ 2(xy + wz) \u0026 1 - 2(x^2 + z^2) \u0026 2(yz - wx) \\\\ 2(xz - wy) \u0026 2(yz + wx) \u0026 1 - 2(x^2 + y^2) \\end{bmatrix}$$ 三维协方差$ \\Sigma $由下式给出：\n$$\\Sigma = RS S^T R^T$$ 其中 $S = \\text{diag}(s) \\in \\mathbb{R}^{3 \\times 3}$。\n高斯分布的深度合成（alpha-blending 计算像素点颜色） 将二维高斯分布划分到 16×16 的瓦片中，并按深度对每个瓦片中的高斯分布进行排序。对于每个高斯分布，我们计算其二维投影协方差（3 sigma）周围的轴对齐边界框，并在其边界框与瓦片相交时将其包括在瓦片中。然后我们应用 [Kerbl et al., 2023] 附录 C 中提出的瓦片排序算法，得到按深度排序的每个瓦片的高斯分布列表。\n步骤\n栅格化每个瓦片中排序后的高斯分布。对于像素 $i$ 的颜色，让 $n$ 索引涉及该像素的 $N$ 个高斯分布：\n$$C_i = \\sum_{n \\leq N} c_n \\cdot \\alpha_n \\cdot T_n， 其中 T_n = \\prod_{m \u003c n} (1 - \\alpha_m)。$$ 我们用二维协方差 $\\Sigma' \\in \\mathbb{R}^{2 \\times 2}$ 和不透明度参数计算 $\\alpha$：\n$$\\alpha_n = o_n \\cdot \\exp(-\\sigma_n)， \\quad \\sigma_n = \\frac{1}{2} \\Delta_n^T \\Sigma'^{-1} \\Delta_n，$$ 其中 $\\Delta \\in \\mathbb{R}^2$ 是像素中心与二维高斯分布中心 $\\mu' \\in \\mathbb{R}^2$ 之间的偏移量。我们在从前到后的过程中计算 $T_n$。\n公式解释\n颜色计算公式：\n$C_i = \\sum_{n \\leq N} c_n \\cdot \\alpha_n \\cdot T_n$，其中 $T_n = \\prod_{m \u003c n} (1 - \\alpha_m)$。\n$c_n$：第 $n$ 个高斯分布的颜色。\n$\\alpha_n$：第 $n$ 个高斯分布的累积不透明度。\n$T_n$：前 $n-1$ 个高斯分布的不透明度积的乘积，表示第 $n$ 个高斯分布的可见度。\n不透明度计算：\n$\\alpha_n = o_n \\cdot \\exp(-\\sigma_n)$\n$o_n$：第 $n$ 个高斯分布的初始不透明度。\n$\\sigma_n$：偏移量的平方距离乘以协方差矩阵的逆。\n偏移量计算：\n$$\\sigma_n = \\frac{1}{2} \\Delta_n^T \\Sigma'^{-1} \\Delta_n$$ $\\Delta_n$：像素中心与高斯分布中心之间的偏移量。\n$\\Sigma'^{-1}$：二维协方差矩阵的逆。\n优化：概述 要从空间中的一堆高斯点获得高质量的图像，需要三个关键组件：良好的初始化、可微分优化和自适应密集化。这些组件可以帮助减少渲染中的尖锐伪影，使图像更平滑和真实。\n初始化\n初始化是指在训练开始时设置三维点的参数。初始化的质量对最终渲染效果至关重要。本文建议使用由 SfM（Structure from Motion）生成的点云来初始化三维点的位置（均值）。SfM 是一种通过分析多张图像来重建三维结构的方法，它可以生成稀疏的点云。\n使用 SfM 生成的点云：\nSfM 通过相机矩阵和多张图像生成三维点云。 这些点云可以用来初始化高斯点的位置，因为它们已经是从真实场景中重建出来的。 随机初始化：\n在初始化时，每个3D点被视为一个球体（即各向同性的协方差矩阵）。 半径的设置基于与相邻点的平均距离，以确保3D世界被适当地覆盖，没有“空洞”。 可微分优化\n在初始化之后，使用简单的随机梯度下降（SGD）来进行优化。场景通过最小化损失函数进行优化，该损失函数是L1损失和结构相似性指数（D-SSIM）损失的组合，用于衡量当前渲染图像与真实图像之间的差异。\n损失函数\nL1 损失：L1损失度量渲染图像和真实图像之间像素值的绝对差异。 D-SSIM 损失：结构相似性指数（SSIM）用于衡量图像的结构相似性。D-SSIM 是其反向度量，用于衡量图像之间的结构不相似性。 自适应密集化\n自适应密集化是优化过程中的一个关键部分，用于解决过度重建和不足重建的问题。自适应密集化在训练期间每隔一段时间（例如每100次SGD步）启动一次。其目的是在现有点无法适当覆盖场景的区域，动态调整点的密度。\n点密集化：\n在具有大梯度的区域分裂点或克隆点。这些区域通常表示高变化率或复杂细节区域，因此需要更多的点来准确表示。对于克隆，创建高斯的复制体并朝着位置梯度移动。对于分裂，用两个较小的高斯替换一个大高斯，按照特定因子减小它们的尺度。 点的剪枝：\n移除那些α值非常低的点。如果一个点的透明度非常高，表示其对最终渲染的贡献很小，因此可以安全地移除这些点以减少计算复杂度。 优化：实现细节（反向传播计算梯度） 给定标量损失 $\\mathcal{L}$ 相对于输出图像每个像素的梯度，我们使用标准链式法则将梯度向后传播到原始输入参数。\nFrobenius 内积\n在下面的推导中，我们将使用 Frobenius 内积来导出矩阵的导数：\n$$\\langle X, Y \\rangle = \\text{Tr}(X^T Y) = \\text{vec}(X)^T \\text{vec}(Y) = \\in \\mathbb{R},$$ 它可以被看作是矩阵点积。Frobenius 内积具有以下性质：\n$$\\begin{aligned} \\langle X, Y \\rangle \u0026= \\langle Y, X \\rangle, \\\\ \\langle X, Y \\rangle \u0026= \\langle X^T, Y^T \\rangle, \\\\ \\langle X, YZ \\rangle \u0026= \\langle Y^T X, Z \\rangle = \\langle X Z^T, Y \\rangle, \\\\ \\langle X, Y + Z \\rangle \u0026= \\langle X, Y \\rangle + \\langle X, Z \\rangle. \\end{aligned}$$ 假设我们有一个标量函数 $f$ 使 $X\\in \\mathbb{R}^{m \\times n}$，且 $X = A$，其中 $A \\in \\mathbb{R}^{m \\times p}$ 和 $Y \\in \\mathbb{R}^{p \\times n}$。我们可以写出 $f$ 相对于任意标量 $x \\in \\mathbb{R}$ 的梯度：\n$$\\frac{\\partial f}{\\partial x} = \\left\\langle \\frac{\\partial f}{\\partial X}, \\frac{\\partial X}{\\partial x} \\right\\rangle,$$ 我们使用简写：\n$$\\partial f = \\left\\langle \\frac{\\partial f}{\\partial X}, \\partial X \\right\\rangle.$$ 这里，$\\frac{\\partial f}{\\partial x} \\in \\mathbb{R}$，$\\frac{\\partial f}{\\partial X} \\in \\mathbb{R}^{m \\times n}$，和 $\\frac{\\partial X}{\\partial x} \\in \\mathbb{R}^{m \\times n}$。\n在这种情况下，继续使用链式法则非常简单。设 $G = \\frac{\\partial f}{\\partial X$，我们有：\n$$\\begin{aligned} \\frac{\\partial f}{\\partial x} \u0026= \\left\\langle G, \\frac{\\partial (AY)}{\\partial x} \\right\\rangle \\\\ \u0026= \\left\\langle G, \\frac{\\partial A}{\\partial x} Y \\right\\rangle + \\left\\langle G, A \\frac{\\partial Y}{\\partial x} \\right\\rangle \\\\ \u0026= \\left\\langle G Y^T, \\frac{\\partial A}{\\partial x} \\right\\rangle + \\left\\langle A^T G, \\frac{\\partial Y}{\\partial x} \\right\\rangle. \\end{aligned}$$ 从这里，我们可以得到 $f$ 相对于 $A$ 和 $Y$ 的梯度的元素：\n$$\\frac{\\partial f}{\\partial A} = G Y^T \\in \\mathbb{R}^{m \\times p}, \\quad \\frac{\\partial f}{\\partial Y} = A^T G \\in \\mathbb{R}^{p \\times n}.$$ 高斯分布深度合成的梯度计算 我们从将像素 $i$ 的损失梯度向后传播到贡献该像素的高斯分布开始。具体来说，对于像素 $i$ 贡献的高斯分布 $i$，我们计算颜色 $\\frac{\\partial \\mathcal{L}}{\\partial c_n} \\in \\mathbb{R^3}$、不透明度 $\\frac{\\partial \\mathcal{L}}{\\partial o_n} \\in \\mathbb{R}$、二维均值 $\\frac{\\partial \\mathcal{L}}{\\partial \\mu_n'} \\in \\mathbb{R}^2$ 和二维协方差 $\\frac{\\partial \\mathcal{L}}{\\partial \\Sigma_n'} \\in \\mathbb{R}^{2 \\times 2}$ 的梯度。\n对于每个通道 $k$的颜色，我们有：\n$$\\frac{\\partial C_i(k)}{\\partial c_n(k)} = \\alpha_n \\cdot T_n$$ 我们保存正向传播过程中计算的最终 $T_N$ 值，并在反向传播过程中计算下一个 $T_{n-1}$ 值：\n$$T_{n-1} = \\frac{T_n}{1 - \\alpha_{n-1}}$$ 对于每个通道 $k$上 $\\alpha$ 的梯度，我们有标量梯度：\n$$\\frac{\\partial C_i(k)}{\\partial \\alpha_n} = c_n(k) \\cdot T_n - \\frac{S_n(k)}{1 - \\alpha_n}$$ 其中，\n$$S_n = \\sum_{m\u003en} c_m \\alpha_m T_m$$ 我们可以在反向传播过程中计算 $S_{n-1}$：\n$$S_N(k) = 0$$ $$S_{n-1}(k) = c_n(k)\\alpha_n T_n + S_n(k)$$ 对于不透明度$o$和 $\\sigma$：\n我们有标量梯度：\n$$\\frac{\\partial \\alpha_n}{\\partial o_n} = \\exp(-\\sigma_n), \\quad \\frac{\\partial \\alpha_n}{\\partial \\sigma_n} = -o_n \\exp(-\\sigma_n)$$ 对于二维均值：\n我们有雅可比矩阵：\n$$\\frac{\\partial \\sigma_n}{\\partial \\mu_n'} = \\frac{\\partial \\sigma_n}{\\partial \\Delta_n} = \\Sigma_n'^{-1} \\Delta_n \\in \\mathbb{R}^2$$ 对于二维协方差：\n我们令 $Y = \\Sigma_n'^{-1}$，其雅可比矩阵从 $\\sigma_n$ 直接得出：\n$$\\frac{\\partial \\sigma_n}{\\partial Y} = \\frac{1}{2} \\Delta_n \\Delta_n^T \\in \\mathbb{R}^{2 \\times 2}$$ 为了继续通过 $Y \\in \\mathbb{R}^{2 \\times 2}$ 进行反向传播，我们令 $G = \\frac{\\partial \\sigma_n}{\\partial Y}$ 并写出相对于标量变量 $x$ 的梯度：\n$$\\frac{\\partial \\sigma_n}{\\partial x} = \\langle G, \\frac{\\partial Y}{\\partial x} \\rangle$$ 我们使用 [Petersen et al., 2008, Dwyer and McPhail, 1948] 的等式，得到：\n$$\\begin{aligned}\\frac{\\partial \\sigma_n}{\\partial x} = \\langle G, -Y \\frac{\\partial \\Sigma_n'^{-1}}{\\partial x} Y \\rangle \\\\ = \\langle -Y^T G Y^T, \\frac{\\partial \\Sigma_n'}{\\partial x} \\rangle \\end{aligned}$$ 因此，相对于 $\\Sigma_n$ 的梯度为：\n$$\\frac{\\partial \\sigma_n}{\\partial \\Sigma_n'} = -\\frac{1}{2} \\Sigma_n'^{-1} \\Delta_n \\Delta_n^T \\Sigma_n'^{-1}$$ 高斯分布投影的梯度计算（2D-\u0026gt;3D） 给定损失函数 $\\mathcal{L}$ 相对于投影后的二维均值 $\\mu$ 和协方差 $\\Sigma$ 的梯度，我们可以继续反向传播单个高斯分布的三维均值 $\\m$ 和协方差 $\\Sigm$ 的梯度。在此，我们一次只处理一个高斯分布，因此省略下标 𝑛，并通过$\\begin{array}{l}\\frac{\\partial\\mathcal{L}}{\\partial\\mu'}\\in\\mathbb{R}^2，\\frac{\\partial\\mathcal{L}}{\\partial\\Sigma'}\\in\\mathbb{R}^{2\\times2}\\end{array}$计算梯度$\\begin{array}{l}\\frac{\\partial\\mathcal{L}}{\\partial\\mu}\\in\\mathbb{R}^3，\\frac{\\partial\\mathcal{L}}{\\partial\\Sigma}\\in\\mathbb{R}^{3\\times3}\\end{array}$\n计算二维均值 $\\mu$ 对相机坐标 $t \\in \\mathbb{R}^4$ 和二维协方差 $\\Sigma'$ 对三维协方差 $\\Sigma$ 及相机坐标 $t$的梯度贡献。\n注意，$\\mu$ 和 $\\Sigma$ 都对 $t$ 的梯度有贡献\n$$\\frac{\\partial\\mathcal{L}}{\\partial t_i}=\\frac{\\partial\\mathcal{L}_{\\mu^{\\prime}}}{\\partial t_i}+\\frac{\\partial\\mathcal{L}_{\\Sigma^{\\prime}}}{\\partial t_i}=\\frac{\\partial\\mathcal{L}}{\\partial\\mu^{\\prime}}\\frac{\\partial\\mu^{\\prime}}{\\partial t_i}+\\langle\\frac{\\partial\\mathcal{L}}{\\partial\\Sigma^{\\prime}},\\frac{\\partial\\Sigma^{\\prime}}{\\partial t_i}\\rangle$$ 对于二维均值 $\\mu$，我们有：\n$$\\frac{\\partial\\mathcal{L}_{\\mu^{\\prime}}}{\\partial t}=\\frac12P^\\top\\begin{bmatrix}w/t_w\u00260\u00260\u0026-w\\cdot t_x/t_w^2\\\\0\u0026h/t_w\u00260\u0026-w\\cdot t_y/t_w^2\\end{bmatrix}^\\top\\frac{\\partial\\mathcal{L}}{\\partial\\mu^{\\prime}}$$ 对于二维协方差 $\\Sigma'$ 对 $\\Sigma$ 和 $t$ 的梯度贡献，$\\Sigma'=T\\Sigma T^\\top$。设 $G = \\frac{\\partial \\mathcal{L}}{\\partial \\Sigma'}$，我们有：\n$\\begin{aligned}\\partial\\mathcal{L}_{\\Sigma^{\\prime}}\u0026=\\langle G,\\partial\\Sigma^{\\prime}\\rangle\\\\\u0026=\\langle G,(\\partial T)\\Sigma T^\\top+T(\\partial\\Sigma)T^\\top+T\\Sigma(\\partial T^\\top)\\rangle\\\\\u0026=\\langle GT\\Sigma^\\top,\\partial T\\rangle+\\langle T^\\top GT,\\partial\\Sigma\\rangle+\\langle G^\\top T\\Sigma,\\partial T\\rangle\\\\\u0026=\\langle GT\\Sigma^\\top+G^\\top T\\Sigma,\\partial T\\rangle+\\langle T^\\top GT,\\partial\\Sigma\\rangle.\\end{aligned}$ 计算相对于协方差矩阵 Σ 的梯度，\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\Sigma} = T^T \\frac{\\partial \\mathcal{L}}{\\partial \\Sigma'} T$$ 我们继续通过 $T = J R_{cw} \\in \\mathbb{R}^{2 \\times 3}$ 传播梯度，对于 $J$ 的梯度，令：\n$$\\partial\\mathcal{L}=\\langle\\frac{\\partial\\mathcal{L}}{\\partial T},(\\partial J)R_{\\mathrm{cw}}\\rangle=\\langle\\frac{\\partial\\mathcal{L}}{\\partial T}R_{\\mathrm{cw}}^\\top,\\partial J\\rangle,\\quad\\mathrm{where~}\\frac{\\partial\\mathcal{L}}{\\partial T}=\\frac{\\partial\\mathcal{L}}{\\partial\\Sigma^{\\prime}}T\\Sigma^\\top+\\frac{\\partial\\mathcal{L}}{\\partial\\Sigma^{\\prime}}^\\top T\\Sigma$$ 我们继续通过 $J$ 对相机坐标 $t \\in \\mathbb{R}^4$ 的贡献进行反向传播：\n$\\frac{\\partial J}{\\partial t_x}=\\begin{bmatrix}0\u00260\u0026-f_x/t_z^2\\\\0\u00260\u00260\\end{bmatrix},\\quad\\frac{\\partial J}{\\partial t_y}=\\begin{bmatrix}0\u00260\u00260\\\\0\u00260\u0026-f_y/t_z^2\\end{bmatrix},\\quad\\frac{\\partial J}{\\partial t_z}=\\begin{bmatrix}-f_x/t_z^2\u00260\u00262f_xt_x/t_z^3\\\\0\u0026-f_y/t_z^2\u00262f_yt_y/t_z^3\\end{bmatrix},\\quad\\frac{\\partial J}{\\partial t_w}=\\mathbf{0}^{2\\times3}$\n我们现在可以将两个梯度 $\\frac{\\partial \\mathcal{L_{\\mu'}}}{\\partial t}$ 和 $\\frac{\\partial \\mathcal{L_{\\Sigma'}}}{\\partial t}$ 合并为 $G = \\frac{\\partial \\mathcal{L}}{\\partial t}$ 并计算相对于三维均值 $\\mu$和视图矩阵 $T_{cw}$ 的全梯度。且有$t=T_\\text{cw}q,\\text{ where }q=\\begin{bmatrix}\\mu\u00261\\end{bmatrix}^\\top$\n$$\\begin{aligned}\\partial\\mathcal{L}\u0026=\\langle G,\\partial t\\rangle=\\langle G,\\partial(T_\\text{cw}q)\\rangle\\\\\u0026=\\langle Gq^\\top,\\partial T_{\\mathrm{cw}}\\rangle+\\langle T_{\\mathrm{cw}}^\\top G,\\partial q\\rangle\\end{aligned}$$ 相对于 $\\mu$ 和 $T_{cw}$ 的梯度：\n$$\\frac{\\partial\\mathcal{L}}{\\partial T_{\\mathrm{cw}}}=\\frac{\\partial\\mathcal{L}}{\\partial t}q^\\top\\in\\mathbb{R}^{4\\times4},\\quad\\frac{\\partial\\mathcal{L}}{\\partial\\mu}=R_{\\mathrm{cw}}^\\top\\begin{bmatrix}\\frac{\\partial\\mathcal{L}}{\\partial t_x}\u0026\\frac{\\partial\\mathcal{L}}{\\partial t_y}\u0026\\frac{\\partial\\mathcal{L}}{\\partial t_z}\\end{bmatrix}^\\top\\in\\mathbb{R}^3$$ 尺度和旋转梯度\n现在我们有 $\\Sigma = M M^3$ 和 $\\frac{\\partial \\mathcal{L}}{\\partial \\Sigma} $。设 $G = \\frac{\\partial \\mathcal{L}}{\\partial \\Sigma}$，我们有：\n$$\\begin{aligned}\\partial\\mathcal{L}\u0026=\\langle G,\\partial\\Sigma\\rangle\\\\\u0026=\\langle G,(\\partial M)M^\\top+M(\\partial M^\\top)\\rangle\\\\\u0026=\\langle GM+G^\\top M,\\partial M\\rangle\\end{aligned}$$ 这给我们：\n$$\\frac{\\partial \\mathcal{L}}{\\partial M} = \\frac{\\partial \\mathcal{L}}{\\partial \\Sigma} M + \\frac{\\partial \\mathcal{L}}{\\partial \\Sigma}^T M $$ 现在我们有 $M = R $，并且 $G = \\frac{\\partial \\mathcal{L}}{\\partial M}$，所以：\n$$\\begin{aligned}\\partial\\mathcal{L}\u0026=\\langle G,\\partial M\\rangle\\\\\u0026=\\langle G,(\\partial R)S\\rangle+\\langle G,R(\\partial S)\\rangle\\\\\u0026=\\langle GS^\\top,\\partial R\\rangle+\\langle R^\\top G,\\partial S\\rangle\\end{aligned}$$ 这给我们：\n$$\\frac{\\partial \\mathcal{L}}{\\partial R} = \\frac{\\partial \\mathcal{L}}{\\partial M} S^T, \\quad \\frac{\\partial \\mathcal{L}}{\\partial S} = R^T \\frac{\\partial \\mathcal{L}}{\\partial M} $$ 旋转矩阵 $R$ 关于四元数参数 $q = (w, x, y, z)$ 的雅可比矩阵是：\n$$\\frac{\\partial R}{\\partial w} = 2 \\begin{bmatrix} 0 \u0026 -z \u0026 y \\\\ z \u0026 0 \u0026 -x \\\\ -y \u0026 x \u0026 0 \\end{bmatrix}, \\quad \\frac{\\partial R}{\\partial x} = 2 \\begin{bmatrix} 0 \u0026 y \u0026 z \\\\ y \u0026 -2x \u0026 -w \\\\ z \u0026 w \u0026 -2x \\end{bmatrix}$$ $$\\frac{\\partial R}{\\partial y} = 2 \\begin{bmatrix} -2y \u0026 x \u0026 w \\\\ x \u0026 0 \u0026 z \\\\ w \u0026 z \u0026 -2y \\end{bmatrix}, \\quad \\frac{\\partial R}{\\partial z} = 2 \\begin{bmatrix} -2z \u0026 -w \u0026 x \\\\ w \u0026 -2z \u0026 y \\\\ x \u0026 y \u0026 0 \\end{bmatrix}$$ 尺度矩阵 $S$ 关于尺度参数 $s = (s_x, s_y, s_z)$ 的雅可比矩阵是：\n$$\\frac{\\partial S}{\\partial s_j} = \\delta_{ij}$$ 其中选择相应的对角元素 $\\frac{\\partial \\mathcal{L}}{\\partial S}$。\nEX：球谐函数 球谐函数在3DGS中并不是必须的\n球谐函数（Spherical Harmonics, SH）\n球谐函数被用来表示视角依赖的颜色，这样可以更好地处理非朗伯反射（如金属表面的镜面反射）。具体来说，通过限制自由度 $\\ell_{\\text{max}}$，每个颜色（红、绿、蓝）可以表示为前$ \\ell\\_{\\text{max}}$ 个球谐函数的线性组合。\n球谐函数是一组定义在球面上的特殊函数，通过选择正整数 $\\ell$ 和 $-\\ell \\leq m \\leq \\ell$ 的一对 $(\\ell, m)$，可以从一个通用公式中导出这些函数。\n公式解释\n球谐函数的通用公式为：\n$$Y_\\ell^m (\\theta, \\phi) = (-1)^m \\sqrt{\\frac{(2\\ell + 1)(\\ell - m)!}{4\\pi (\\ell + m)!}} P_\\ell^m (\\cos \\theta) e^{im\\phi}$$ 其中：\n$\\ell $和 m 是整数，$\\ell \\geq 0，-\\ell \\leq m \\leq \\ell$。\n$\\theta$ 是极角（通常在0到$\\pi$之间），$\\phi$ 是方位角（通常在0到$2\\pi$之间）。\n$P_\\ell^m$ 是缔合勒让德多项式（Associated Legendre Polynomials）。\n球谐函数的性质\n正交性： 球谐函数是正交的，这意味着在球面上的任意两个不同的球谐函数在积分意义下相互独立。 归一化： 球谐函数是归一化的，因此可以形成球面上函数空间的正交基。 简化： 对于小的$\\ell $值，球谐函数公式会显著简化。例如，当 $\\ell $ = 0 时，球谐函数是一个常数，当 $\\ell $ = 1 时，球谐函数也是相对简单的形式。 颜色表示\n对于每个三维高斯点，我们希望学习正确的系数，使得从某个方向看该三维点时，它传达的颜色最接近真实颜色。这是通过以下步骤实现的：\n选择最大自由度：\n选择一个适当的$ \\ell\\_{\\text{max}}$ 值，以限制球谐函数的数量。 线性组合：\n每种颜色（红、绿、蓝）都表示为前$ \\ell\\_{\\text{max}} $个球谐函数的线性组合。对于每个三维高斯点，学习这些线性组合的系数。 视角依赖的颜色计算：\n给定一个观察方向，使用球谐函数和学习到的系数计算该方向上的颜色。 示例\n假设我们选择 $\\ell_{\\text{max}}=2$，则有 5 个球谐函数（$\\ell $= 0, 1, 2 对应的各个 m 值）。我们需要为每个颜色学习 5 个系数。假设对于某个高斯点，这些系数为 $c_{r,i}, c_{g,i}, c_{b,i}$（红、绿、蓝）。\n对于一个特定的观察方向 $(\\theta, \\phi)$，我们可以计算该方向上的颜色：\n$$\\text{Color}(\\theta, \\phi) = \\left( \\sum_{i=1}^5 c_{r,i} Y_i(\\theta, \\phi), \\sum_{i=1}^5 c_{g,i} Y_i(\\theta, \\phi), \\sum_{i=1}^5 c_{b,i} Y_i(\\theta, \\phi) \\right)$$ 总结\n球谐函数提供了一种有效的方法来表示视角依赖的颜色，使得模型能够处理非朗伯反射效果。在具体实现中，通过选择适当的 $\\ell_{\\text{max}}$，并学习每个颜色的球谐函数系数，可以实现高质量的渲染效果。球谐函数的正交性和归一化性质保证了这种表示的数学稳健性和计算效率。 资源消耗 数据准备（快） 使用COLMAP从图像集合中提取SfM信息 训练（较慢） GPU NVIDIA RTX 4090 显存 24 GB 场景 drjohnson playroom bottle scene(dynamic) 图片数量 263 225 48 105 图片分辨率(px) 1332x876 1264x832 1081x1932 1065x1895 训练时间 41 min 17 min 17 min 22 min .ply 文件大小（7000 次迭代） 423 MB（1,789,615个顶点） 403MB（1,618,690个顶点） 48MB 136MB .ply 文件大小（30000 次迭代） 812 MB（3,433,974个顶点） 587MB（2,356,284个顶点） 54MB 324MB 1个顶点约0.25KB\n高斯核均值信息$\\mu$：x, y, z 位置信息 协方差矩阵$\\Sigma$：3x3矩阵，表示高斯核的缩放+旋转 透明度$\\alpha$ SH球谐函数颜色信息：48个（只用到前4阶） 渲染（快） GPU Apple M1 Pro Render window 1920x1080（1080p） 2268x1420（~2K） FPS（2M～3M顶点） 50～60 FPS 20～30 FPS FPS（1M～2M顶点） 60+ FPS 30～40 FPS 由于mac不支持3DGS源代码提供的SIMR viewer，因此目前采用的是基于WebGL的方式（SuperSplat，Three.js），如果在windows平台上通过官方渲染程序运行应该会更快。\n数据集 数据集名称 描述 来源 LLFF 本地光场融合（LLFF）数据集包括自然场景的合成图像和真实图像。合成图像由SUNCG和UnrealCV生成，而真实图像包括使用手持手机拍摄的24个场景。 链接 NeRF 神经辐射场（NeRF）数据集包含复杂场景的合成渲染和真实图像。它包括漫射合成360°、真实合成360°和复杂场景的真实图像。 链接 DONeRF DONeRF数据集包括使用Blender和Cycles路径追踪器生成的合成数据，每个场景渲染300张图像。 链接 X3D X3D数据集包括15个专用于X射线3D重建的场景，涵盖医学、生物学、安全和工业应用。 链接 RTMV RTMV是一个用于新视图合成的合成数据集，包括通过光线追踪生成的300,000张图像，涵盖2,000个场景。 链接 Tanks\u0026amp;Temples Tanks\u0026amp;Temples数据集是综合性的，提供了用于基于图像的3D重建管道的中级和高级测试数据集。 链接 RealEstate10K RealEstate10K是一个大型数据集，从10,000个YouTube视频中获取相机位姿，提供通过SLAM和捆绑调整算法获得的轨迹。 链接 ACID 空中海岸线影像数据集（ACID）数据集专注于基于单一图像的延长相机轨迹生成新视图，采用几何和图像合成的混合方法。 链接 SWORD \u0026lsquo;包含遮挡区域的场景\u0026rsquo;数据集（SWORD）包含1,500个训练视频和290个测试视频，强调用于鲁棒模型训练的近物体和遮挡。 链接 Mip-NeRF 360 Mip-NeRF 360数据集扩展了Mip-NeRF，具有非线性参数化、在线非蒸馏和用于无边界场景的畸变基正则化器。 链接 Deep Blending 用于自由视点基于图像渲染的深度混合数据集包括9个场景，这些场景使用立体相机装备捕获并使用COLMAP和RealityCapture重建。 链接 DTU DTU数据集是多视点立体数据，具有精确的相机定位、结构光扫描仪和不同照明条件的多样化场景。 链接 ScanNet ScanNet是一个室内RGB-D数据集，包含1513个标注扫描，提供90%的表面覆盖率和多样化的3D场景理解任务。 链接 ShapeNet ShapeNet是一个大型3D CAD模型库，对NeRF模型来说非常有价值，强调基于对象的语义标签。 链接 Matterport 3D Matterport-3D数据集包括来自90个建筑规模场景的10,800个全景图视图，具有深度、语义和实例注释。 链接 Replica Replica数据集是一个真实的室内数据集，包含18个场景和35个房间，具有手动调整、语义注释以及基于类和基于实例的标签。 链接 Plenoptic Video 全光视频数据集包含使用全光相机捕获的3D视频，提供逼真和沉浸式的3D体验。 链接 Panoptic CMU全景数据集包含超过150万个实例的3D姿态注释，这些实例出现在社交活动中，并使用同步摄像机捕获，场景多样。 链接 研究现状 功能性 拓展3DGS本身的能力\n动态和变形 拓展的参数列表 在时间 $ t$的 3D 位置：$[x(t), y(t), z(t)]^\\top \\in \\mathbb{R}^3 $ 在时间 $ t$的 3D 旋转，由四元数表示：$[q_x(t), q_y(t), q_z(t), q_w(t)]^\\top \\in \\mathbb{R}^4 $ 缩放因子：$[s_x, s_y, s_z]^\\top \\in \\mathbb{R}^3 $ 表示颜色的球谐系数，具有自由度 k：$h \\in \\mathbb{R}^{3 \\times (k + 1)^2} $ 不透明度：$o \\in \\mathbb{R} $ 动态场景追踪\nDynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis\n4D Gaussian Splatting for Real-Time Dynamic Scene Rendering（CVPR 2024）\n动态场景编辑\nControl4D: Efficient 4D Portrait Editing with Text（CVPR 2024）\nSC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes（CVPR 2024） 扩散模型 文生3D\nGaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models（CVPR 2024）\nGsgen: Text-to-3D using Gaussian Splatting（CVPR 2024）\nDreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation（ICLR 2024 Oral）\n去噪和优化\nGaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion Probabilistic Models with Structured Noise 利用高斯 Splatting 和 Langevin 动力学扩散模型可加速加速渲染并提高真实感。 优化和加速 Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images Compact 3D Gaussian Representation for Radiance Field EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS COLMAP-Free 3D Gaussian Splatting 渲染和着色 Mip-Splatting: Alias-free 3D Gaussian Splatting Relightable 3D Gaussian: Real-time Point Cloud Relighting with BRDF Decomposition and Ray Tracing GS-IR: 3D Gaussian Splatting for Inverse Rendering Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering GaussianShader: 3D Gaussian Splatting with Shading Functions for Reflective Surfaces Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting 压缩 LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS Compact3D: Compressing Gaussian Splat Radiance Field Models with Vector Quantization 应用性 3DGS的实际应用\n数字化身 基于铰接式或联合式 可动画化 基于头部 SLAM GS-SLAM SplaTAM: Splat, Track \u0026amp; Map 3D Gaussians for Dense RGB-D SLAM Gaussian Splatting SLAM Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting Mesh 提取 + 物理 通过在 3D 高斯中添加更多参数 核速度、应变和其他物理特性可以被建模。\nPhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics MD-Splatting: Learning Metric Deformation from 4D Gaussians in Highly Deformable Scenes 可编辑性 GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting Point\u0026rsquo;n Move: Interactive Scene Object Manipulation on Gaussian Splatting Radiance Fields Gaussian Grouping: Segment and Edit Anything in 3D Scenes Segment Any 3D Gaussians Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields 对比 总结 发展方向 交互探索：实时3D重建技术允许实时交互探索3D场景或模型，并提供即时反馈。 动态渲染：实现动态场景中移动物体或变化环境的实时渲染，增强真实感和沉浸感。 模拟与培训：在汽车、航空航天和医学等领域的模拟和训练环境中提供逼真的视觉反馈。 AR**/VR体验**：支持沉浸式AR和VR体验的实时渲染，使用户能够实时与虚拟物体或环境互动。 技术优势：提高了计算机图形学、可视化、模拟和沉浸式技术中各应用的效率、互动性和真实性。 高斯散点技术（Gaussian Splatting）在处理动态场景、交互式对象操作、3D分割和场景编辑方面有着广泛的潜在应用和未来发展方向。该技术具有广泛而深远的应用前景，分布在多个领域，包括计算机生成图像（CGI）、虚拟现实/增强现实（VR/AR）、机器人技术、电影与动画、汽车设计、零售、环境研究和航空航天应用等。然而，重要的是要注意到，高斯散点在实现照片级真实性方面可能不及其他方法，比如NeRF（神经辐射场）。\n局限性 尽管高斯分布渲染（Gaussian Splatting）在整体上取得了出色的结果和令人印象深刻的渲染速度，但这种表示方法的简单性也带来了一些代价。以下是主要的局限性和需要考虑的问题：\n密集化启发式 在优化过程中，引入了各种密集化启发式，以防止模型出现“破损”的高斯点（如过大、过长或冗余的点）。这些密集化措施对于保持模型的稳定性和一致性至关重要。如果没有这些措施，模型可能会在优化过程中产生问题。\n过大或过长的高斯点：如果高斯点的尺寸过大或过长，可能会导致渲染结果失真。 冗余的高斯点：过多的冗余点会增加计算复杂度，而不会显著提高渲染质量。 这些问题在处理超出新视角渲染任务范围的其他任务时可能会进一步放大。\n离散表示的选择 选择离散表示而非连续表示意味着丧失了多层感知机（MLP）的归纳偏置。在 NeRFs 中，MLP 执行隐式插值，平滑处理视角之间的可能不一致性，而三维高斯点对这些不一致性更加敏感，导致上述问题的出现。\nMLP 的插值和平滑：在 NeRF 中，MLP 可以通过插值和平滑减少视角之间的差异。 三维高斯点的敏感性：三维高斯点在处理这些不一致性时更容易出现问题，导致渲染结果不如 NeRF 平滑。 继承的伪影问题 高斯分布渲染继承了一些 NeRF 中存在的已知伪影，这些伪影源于共享的图像形成模型。例如：\n在较少或未见区域的较低质量：在训练数据中较少或未出现的区域，渲染质量可能较低。 靠近图像平面的浮动伪影：在靠近图像平面的区域，可能会出现浮动伪影。 这些问题在高斯分布渲染和 NeRF 中都是存在的，源于它们使用的相似图像形成模型。\n检查点文件大小 检查点文件的大小是另一个需要考虑的属性。尽管新视角渲染尚未被部署到边缘设备，但从磁盘空间的角度来看，三维点的数量和流行的 NeRF MLP 架构占用了相同数量级的磁盘空间，平均而言，高斯分布渲染的文件大小比 NeRF 略大几倍。\n磁盘空间占用：高斯分布渲染由于三维点的数量较多，文件大小略大于 NeRF。 部署考虑：虽然目前部署到边缘设备的需求不大，但未来可能需要考虑文件大小对部署的影响。 参考资料 3DGS概述 ⭐️ 3D Gaussian Splatting原理速通（一）～（四）（29 min watch） ⭐️ Gaussian Splatting is pretty cool!（10 min read） ⭐️ Understanding and Exploring 3D Gaussian Splatting: A Comprehensive Overview（9 min read） 3DGS 官方 Tutorial （2 hours watch） NeRF坑浮沉记3D Gaussian Splatting入门（5 min read） 一文带你入门 3D Gaussian Splatting（10 min read） 原理详解 ⭐️ A Comprehensive Overview of Gaussian Splatting（12 min read） Mathematical Supplement for the gsplat Library（30 min read） NumByNum 3D Gaussian Splatting Reviewed（29 min read） EWA Splatting （30+ min read） 研究现状 ","permalink":"/posts/3dgs/","summary":"3DGS 基本思想 3D高斯分布可以通过它们的各向异性协方差矩阵、位置和透明度等参数来有效地表示复杂场景。由于这些参数是通过机器学习方法进行训练的，渲染阶段无需进行大量处理。因此，它可以利用基于瓦片的光栅化器实现快速渲染，从而在性能上有显著的提升。 创新点 Point-Based Rendering：点基渲染直接将三维空间中的点渲染为图像。 Tiled Rasterization：分块光栅化的基本思想是将屏幕划分为多个小块（Tiles），然后在每个小块内进行相关计算和处理（可微分）。这种方法能够显著减少内存流量，从而提高渲染效率。 Spherical Harmonics：球谐函数是一种在球面上表示函数的方法，特别适用于描述球形表面的光照和阴影效果。 基本流程 收集数据\n图像\n视频-\u0026gt;ffmpeg截取视频帧\nffmpeg -i \u0026lt;VIDEO_PATH\u0026gt; -qscale:v 1 -qmin 1 -vf fps=2 %04d.jpg 输出如下\n📦 $FOLDER_PATH ┣ 📂 input ┃ ┣ 📜 000000.jpg ┃ ┣ 📜 000001.jpg ┃ ┣ 📜 ... 获取相机位姿\nCOLMAP：开源Structure-from-Motion (SfM) 软件，输入images，输出相机位姿\n原论文使用的是自带的convert.py，自动调用COLMAP并转换成需要的格式\n桌面软件：RealityCapture, Metashape\n移动app：Polycam, Record3D（利用了雷达）\n输出如下：\n📦 $FOLDER_PATH ┣ 📂 (input) ┣ 📂 (distorted) ┣ 📂 images ┣ 📂 sparse ┃ ┣ 📂 0 ┃ ┃ ┣ 📜 points3D.","title":"3DGS Tutorial"},{"content":" 原文链接\nQ1: how to learn image representations? Overview 改进 CLIP 数据层面：Data scaling up 模型层面：Model design image side\nFLIP（Scaling CLIP training via masking）：是一种改进的训练方法，用于提高 CLIP模型的训练效率。FLIP 的核心思想是在训练过程中随机遮挡图像的部分区域，只对可见的区域进行编码。\nScaling language-image pre-training via masking, CVPR 2023\nlanguage side\nK-Lite: 将外部知识融入到对比学习预训练中，在 K-Lite 中，实体的维基百科定义（knowledge）可以与原始的图像替代文本（alt-text）一起自然地用于对比预训练。\nK-lite: Learning transferable visual models with external knowledge, NeurIPS 2022\nimproved interpretability\nSTAIR（Learning Sparse Text and Image Representation in Grounded Tokens）： 将图像和文本映射到高维稀疏嵌入空间； 每个维度的值是一个非负标量，表示与该维度对应的词或标记的权重； 提供了更好的性能和更清晰地识别图像和文本之间的对应关系； STAIR: Learning Sparse Text and Image Representation in Grounded Tokens, 2023\nmore modalities ImageBind（One embedding space to bind them all）：将不同模态（在这篇论文中有7种模态）链接到一个共同空间的方法。 一个共同的嵌入空间。 使用冻结的预训练 CLIP。 学习其他模态编码器：对于每个非图像和文本的模态，ImageBind 学习一个编码器，将该模态的数据映射到 CLIP 的嵌入空间中。 目标函数：Objective function fine-grained supervision（细粒度监督） FILIP：通过细粒度监督来改进视觉-语言预训练的方法。即单词-图像块（patch）对齐。 双编码器结构：FILIP 仍然使用双编码器结构，即分别对图像和文本进行编码，而不是使用融合编码器。 细粒度监督：FILIP 的核心在于细粒度的监督。它首先计算单词和图像块之间的相似性，然后通过最大池化（max pooling）聚合这个相似性矩阵来计算损失。这种方法允许模型学习更细粒度的图像-文本对齐。 学习单词-图像块对齐：通过这种细粒度的相似性计算和损失函数，FILIP 能够学习单词和图像块之间的对齐关系。这种对齐对于可视化非常有用，因为它提供了图像和文本之间更明确的关联。 FILIP: Fine-grained Interactive Language-Image Pre-Training, ICLR 2022\nadding a generative branch Coca：通过在预训练阶段同时利用图像-文本对和图像-标签对来学习图像和文本的表征。并增加了一个生成分支，以提升模型性能并赋予其新的能力，如图像描述和视觉问答。 对比学习：CoCa 使用对比学习机制来学习图像和文本之间的对应关系，通过最大化相关图像-文本对的相似度，同时最小化不相关对的相似度。 混合数据预训练：CoCa 利用混合的图像-文本对和图像-标签对进行预训练。这种混合使用使模型能够同时学习丰富的视觉概念和复杂的跨模态关系。 生成分支：CoCa 在传统的对比学习框架上增加了一个生成分支。这个分支可以生成文本描述，从而增强模型的性能，并使模型能够执行图像描述和视觉问答等任务。 学习从零开始的图像编码器：Coca完全从零开始训练，以更深层次地理解视觉内容。 Coca: Contrastive captioners are image-text foundation models, 2022\nCLIP + 其他学习方法 CLIP + Supervised Learning Noisy label + text supervision（噪声标签+文本监督）\nUniCL ：它提供了一种原则性的方法来同时使用图像-标签和图像-文本数据。它是一种统一的对比学习（Unified Contrastive Learning）框架，旨在优化图像、文本和标签在同一空间内的表征。\nLiT: Locking the image encoder：使用预训练好的image encoder并将它冻结，并添加文本塔实现开放词汇表。使text model学会从image encoder的结果中读取出好的表示。\nMOFI：从带有噪声的实体标注图像中学习图像表示的方法。它结合了监督学习和对比学习，以提高模型在多任务环境中的性能。\nUnified contrastive learning in image-text-label space, CVPR 2022\nLit: Zero-shot transfer with locked-image text tuning, CVPR 2022\nMOFI: Learning Image Representations from Noisy Entity Annotated Images, 2023\nCLIP + Image-Only (Non-) Contrastive Learning SimCLR：对于给定的图像，应用两种不同的数据增强方法，生成两个变体。这两个变体被视为正样本对，即它们代表同一图像的不同视角，并进行对比训练。\nDeCLIP：\n自监督学习： 图像模态：使用类似 SimCLR 的方法，通过双重数据增强和对比学习来学习图像的特征表示。 文本模态：使用掩码语言模型（MLM）的方式来学习文本的特征表示。 多视角监督（Multi-view supervision）：利用来自不同模态（如图像和文本）的信息作为互补的视角，通过对齐这些不同视角的表示来增强学习。 最近邻监督（Nearest-neighbor supervision）：在特征空间中，通过考虑样本的最近邻来引入额外的监督信号，从而促进模型学习更加鲁棒和区分性的特征。 SLIP：SLIP 的核心思想是将 SimCLR和 CLIP相结合进行模型训练。\nxCLIP： xCLIP = CLIP + nCLIP，是对 CLIP 的一个扩展，它利用图像自监督学习的技术来实现非对比学习，通过正则化和高维投影来确保学到的表示具有足够的区分度和鲁棒性。 引入了锐度（sharpness）和平滑性（smoothness）正则化。锐度正则化鼓励模型学习到尖锐的、区分度高的特征，而平滑性正则化则确保模型不会对数据的微小变化过度敏感。\nA Simple Framework for Contrastive Learning of Visual Representations, ICML 2020\nSupervision exists everywhere: A data efficient contrastive language-image pre-training paradigm, ICLR 2022\nSlip: Self-supervision meets language-image pretraining, ECCV 2022\nNon-Contrastive Learning Meets Language-Image Pre-Training, CVPR 2023\nImage-Only (Non-) Contrastive Learning + Masked Image Modeling BEiT：将自然语言处理中的预训练技术引入到图像领域，并通过视觉标记的概念建立起图像和文本之间的联系。 图像分词器（Image Tokenizer）：在预训练之前，首先使用 VQ-VAE或 GAN等方法学习一个“图像分词器”。将图像分割成一系列离散的视觉标记，类似于 DALL-E 和 Parti 等图像生成模型中使用的方法。 随机遮蔽与预测：在预训练阶段，随机遮蔽图像的一些区域（图像块），然后训练 BEiT 模型去预测这些被遮蔽的视觉标记。 知识蒸馏：这个过程可以理解为图像分词器和 BEiT 编码器之间的知识蒸馏，但后者只能看到图像的一部分。BEiT 编码器通过学习重建遮蔽的视觉标记，从而学习到图像的丰富表示。 MAE：使用像素值作为目标来训练模型。 大规模遮蔽：在预训练阶段，随机遮蔽图像的大部分区域（例如 75%），只留下一小部分可见区域。 编码器和遮蔽标记：将编码器应用于可见的图像块，以提取特征。在编码器之后引入遮蔽标记。 像素级重建：模型的任务是预测被遮蔽区域的像素值，从而重建整个图像。 适用于目标检测和分割：MAE 预训练对于目标检测和分割等任务特别有帮助，因为它强化了模型对图像局部细节和整体结构的理解。 MIM很适合模型微调，但不能学习全局图像表示，也不适用大规模数据缩放。\nBEiT: BERT Pre-Training of Image Transformers, ICLR 2022\nMasked Autoencoders Are Scalable Vision Learners, CVPR 2022\nCLIP + Masked Image Modeling （Shallow interaction） CLIP和 MIM是两种不同的自监督学习方法，它们各自专注于不同方面的特征学习。\nMVP：使用 CLIP 的图像特征作为 MIM 训练的目标，可以捕获在MIM训练中缺失的语义，这种做法能够结合两种方法的优势，提升模型在捕捉图像语义信息方面的能力。\nEVA：推广了这一方法\nMasked Autoencoding Does Not Help Natural Language Supervision at Scale：MAE对大规模的自然语言监督没有帮助。\nBEiT-3：结合了 BERT和 BEiT的思想，进行多模态数据的遮蔽建模。\n多模态变换器（Multiway Transformer）：BEiT-3 使用一个多路变换器来处理图像/文本和图像-文本数据。这种架构允许模型同时处理并整合来自不同模态的信息。\n遮蔽数据建模：与单一模态的 MIM 类似，BEiT-3 对图像和文本数据进行遮蔽处理，并训练模型去预测被遮蔽的部分。这种方式促使模型学习到深层次的、跨模态的表示。\n共享自注意力层：BEiT-3 设计了共享的自注意力层，这些层可以处理来自不同模态的信息。这种共享机制有助于提高模型的参数效率和泛化能力。\n模态专家前馈网络（FFN Modality Experts）：模型包含三个前馈网络（FFN）模态专家，分别专注于处理图像、文本和图像-文本数据。这种设计使得模型能够针对不同模态的数据学习特定的特征。\nMVP: Multimodality-guided Visual Pre-training, ECCV 2022\nEVA: Exploring the Limits of Masked Visual Representation Learning at Scale, CVPR 2023\nMasked Autoencoding Does Not Help Natural Language Supervision at Scale, CVPR 2023\nImage as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks\n主干网络（Backbones） 总结 三个高层次的原则 可扩展性（Scaling）：一个好的算法应该简单，但也能够很好地扩展。这意味着算法应该能够处理大规模的数据集和模型，同时保持良好的性能。 对比（Contrasting）：从 SimCLR 到 CLIP，对比学习在自监督学习中扮演着重要角色。通过比较正负样本对，模型学习到区分不同数据点的能力。 遮蔽（Masking）：从 BERT 到 BEiT，遮蔽技术被广泛应用于自监督学习中。通过预测被遮蔽部分的内容，模型能够学习到数据的内在结构和特征。 展望 进一步扩展规模：\n数据规模和模型规模的扩展仍是一个挑战。需要探索更有效的方法来处理和学习超大规模的数据集，以及设计更大更强大的模型架构。 新的模型训练范式：\n寻找超越 CLIP 和 MIM 的简单且可扩展的算法。这可能涉及到新的学习机制、训练策略或模型架构。 统一的图像-/区域-/像素级预训练：\n开发能够在不同粒度级别上对图像进行全面理解的模型。这需要模型能够同时捕捉到全局信息和局部细节。 具有更灵活、可提示（promptable）接口的视觉模型：\n将自然语言处理中的概念（如上下文学习、思维链、提示、新兴属性等）引入到计算机视觉中。探索如何使视觉模型能够通过提示进行灵活的交互和学习。 使用更创新的数据训练视觉主干网络：\n探索新的数据来源和类型，以解锁模型的新能力，类似于 GPT-4 展示的能力。例如，训练模型阅读整个扫描的论文，然后用几个要点来总结论文内容。 Q2: how to extend vision models with more flexible, promptable interfaces? 计算机视觉领域独有的挑战 模型 输入格式多种多样：图片，视频，多模态（附带文本/语音） 不同粒度的任务：图片级，区域级，像素级 不同类型的输出：空间输出，文本输出 数据 如何解决 Bridge Vision with Language 学习原始视觉信号与丰富语义之间的映射，并可以对各种开放词汇视觉识别任务提供动力\nGroupViT：通过从头开始学习图像-文本对来学习对语义相似的区域进行分组 使用分组块进行自下而上的分组 自上而下的图像-文本监督以实现视觉语义对齐 MaskCLIP：从CLIP中提取自由的密集标签 将注意力池更改为新的适应策略 使用CLIP作为教师模型的伪标签掩码 OpenSeg：弱监督学习，通过强制文本特征和掩码池特征之间的细粒度对齐。 从图像-文本对和局部描述中学习。 使用预训练的掩码建议网络。 MaskCLIP (UCSD)：使用CLIP作为初始化的COCO全视分割的监督训练 掩码建议网络训练 CLIP模型自适应 总结 CLIP作为基础对开放词汇训练有很大帮助 将弱注释与黄金注释结合起来以获得更好的性能 Unified multi-task modeling 希望开发一个统一的视觉模型，该模型可以在许多视觉任务中表现良好。\n两种统一化\nOutputs Unification 概述 以一致的格式表示不同输出（一个输出里包含多个任务的结果） UniTab and Pix2Seqv2 统一词表： 将文本和坐标都进行标记化（tokenization），并放入同一个词表中。 任务前缀： 需要一个任务前缀来确定模型正在处理的任务 Unified-IO：加入了VQVAE 预训练 VQVAE： 先单独对 VQVAE 进行预训练，让它学习有效地表示和离散化不同类型的视觉信息。 序列到序列的联合训练： 在预训练的 VQVAE 之上，以序列到序列（Sequence-to-Sequence）的方式对整个 Unified-IO 模型进行联合训练。这样做是为了让模型学习如何处理不同模态的输入，并生成符合不同任务要求的序列形式输出。 总结 优点： 这个方式可以让一个模型通过自然语言的转换来适应各种视觉任务。 局限性： 仍然需要有任务特定的步骤来把大语言模型的输出转化成可用的结构化数据。 中间过程涉及自然语言，因此模型内部不同任务的交互关系可能会更难以解释。 相比直接输出结构化数据的模型，VisionLLM 在整合不同任务、提升整体性能方面可能存在局限性。 Functionality Unification 概述 知识迁移： 一个模型在特定任务上学习到的能力，可以较容易地迁移到使用相似输出类型的其他任务上。 模型简化： 我们可以设计更具有通用性的模型架构，通过输出层的适配来应对不同的视觉任务。 多任务学习： 既然任务之间存在关联，就有机会设计联合学习方案，让模型同时在多个任务上训练，以相互促进，提升整体性能。 UniPerceiver-v2 X-Decoder：允许图像级和像素级的监督信号相互作用。 LLM-like promptable interface 概述：一个通用的视觉模型应该具有相同的上下文学习能力，在不改变模型参数的情况下将输出与各种用户意图对齐。 上下文记忆 （Context Memory）： 模型需要能存储和处理之前交互轮次中的信息。这可能是通过一个外部记忆模块，或者大规模参数模型本身的记忆能力来实现。 提示构建（Prompt Construction）： 系统能够根据：1) 用户的多轮意图输入； 2) 存储的上下文信息来自动构建模型的提示。提示的内容可能包括任务描述、指示、以及之前的示例交互。 SAM SEEM: Segment Everything Everywhere all at Once Q3: how to do image generation? Overview 4大主题\n研究总结\nT2I模型总览\nDF原理\n交叉注意力机制：K和V代表键（Key）和值（Value），它们是从文本流τ(y)投影来的，而Q是从视觉流投影来的，都具有相同的隐藏维度d。因此，Q和K之间的softmax运算产生了一个大小为(hw×d)·(N×d)转置= hw×N的注意力图M。这个交叉注意力图M表示图像和文本之间的细粒度交互，对于文本中的每一个词N，在所有空间位置hw上都有交互。然后注意力图M与V进行点积运算，以产生一个下采样/上采样块的输出。\nSpatial Controllable Generation 区域绑定的文本描述：将传统T2I模型中的图像级文本描述扩展到区域绑定的文本描述，使得开放式文本描述能够精确地操作特定的空间区域。\nReCo：其核心思想是扩展文本编码器E的文本词汇，并安排不同的标记以表示绑定的文本输入。该研究通过增加一组位置标记（例如，\u0026lt;687\u0026gt;、\u0026lt;204\u0026gt;、\u0026lt;999\u0026gt;、\u0026lt;833\u0026gt;），这些位置标记与文本标记无缝混合，并作为空间修饰符，指示接下来的文本仅在指定的空间区域操作。\nGLIGEN：采用了一种即插即用的方法，冻结原始T2I模型，并训练额外的门控自注意力层来学习新的定位技能。定位令牌携带两种类型的信息：需要在其中定位的文本词的语义表示和它们的空间配置。然后，通过新增加的门控自注意力层将这些定位令牌添加到预训练的T2I模型中，所有剩余的预训练参数都被冻结。\nReco: Region-controlled text-to-image generation.\nGligen: Open-set grounded text-to-image generation\n密集空间条件：这一类研究从边界框扩展到以2D数组形式表示的密集空间条件，如分割掩码、边缘图、深度图和关键点等。\nControlNet：向文本提示添加了额外的输入条件。这个额外的条件可以是Canny边缘图、霍夫线、HED边界、素描、人体姿势图、分割掩模、深度图像、法线图或线条图，每个条件都有其独特的模型副本。\nControlNet：Adding conditional control to text-to-image diffusion models.\n推理时指导：前两类工作需要对T2I模型进行微调以理解扩展的空间条件。第三类技术探讨了在不对模型进行微调的情况下实现空间控制的方法。\n核心思想与分类器指导（Classifier guidance）类似，即使用判别器损失来引导扩散过程。具体来说，就是在扩散过程的每一步，加入一个额外的项，这个项是由判别器计算出的目标检测损失与期望布局有关的梯度，乘以一个指导强度因子，这样可以在不额外训练模型的情况下实现空间控制。 Text-based Editing 文本到图像编辑（Text-to-image editing）是一种从给定图像和输入的文本描述中合成新图像的技术。可以是之前从文本到图像模型生成的图像，或者是自然图像。目标是保留大部分视觉内容，只修改特定组成部分。\n局部图像区域变化：经典的编辑场景之一是改变局部图像区域，例如移除、更换或在某个区域内添加对象。 扩展的空间编辑：语言输入描述了空间区域中期望的外观，语言也可以用作编辑指令告诉机器要做什么，例如“将图像中的对象A更改为对象B”。 综合编辑系统：不是扩展单一的文本到图像（T2I）模型进行编辑，编辑系统集成了不同的专业模块，如分割模型和大型语言模型。 Text Prompts Following 文本到图像（T2I）模型可能无法遵循文本提示的问题，尤其是当图像描述变得复杂时。例如，某些名词短语可能被遗漏，属性可能应用于错误的对象，生成的图像可能有错误的对象数量、关系、风格等。\n推理时操作（Inference-time manipulation）:\n在推理阶段，设计各种方法来重新分配视觉潜在表示或图像-文本交叉注意力，以确保文本提示中的所有名词短语都在生成的图像中得到体现。 StructureDiffusion：使用解析树来提取名词短语和文本提示的语言结构，然后强制模型“关注”所有提取的名词短语。这是通过修改交叉注意力机制来实现的，其中 O = M · V，M 是 softmax 交叉注意力图，V 是句子特征。\n$$O=\\frac1{k+1}\\Sigma_{i=0}^k(M\\cdot V_i)$$ ，其中 $$V_0$$ 是句子特征 ，\n$$V_i$$ 是解析树中的短语特征。这种方法确保视觉流在所有识别的名词短语上保持平衡的注意力，从而促进更准确的图像生成。\nAttend-and-Excite：提出了一种正则化损失函数 $$\\begin{aligned}\\ell=\\max_{n=1,...,N_{\\mathrm{sub}}}(1-max\\left.G(M_t^n)\\right)\\end{aligned}$$ ，用于增强最被忽视的主题tokens的最大注意力。该正则化损失函数使用了一个高斯核 G 来平滑注意力地图，\n$$N_{sub}$$ 是主题tokens的数量。然后利用这个损失函数更新在推理时间的潜在表示 zt，更新公式为\n$$z_t^{\\prime}=z_t-\\alpha\\nabla_{z_t}\\ell$$ ，\n$$\\alpha$$ 是步长大小。\nTraining-free structured diffusion guidance for compositional text-to-image synthesis\nAttend-and-excite: Attention-based semantic guidance for text-to-image diffusion models.\n对齐调优（Alignment tuning）:\n学习一个额外的模型学习阶段，通常以图像-文本相似性作为奖励，使得调优后的模型能够更好地遵循文本提示。 DDPO：加入强化学习\nTraining diffusion models with reinforcement learning\nConcept Customization 直接扩展T2I模型以通过图像输入理解视觉概念。\n单一概念定制: 从单一概念定制开始，该过程涉及测试时微调，将视觉概念的多张图片编码成新的Token嵌入。\nTextual Inversion：T2I 模型处理不同狗品种的四个图像，随后学习新标记的嵌入，表示为 [V]。这个 [V] 标记可以用作文本标记来表示这个特定的狗。[V] 令牌可以与其他文本描述无缝集成。文本反转通过前缀调整来学习 [V] 令牌嵌入，即冻结所有 T2I 模型的参数并训练 [V] 令牌嵌入以生成输入图像。\nDreambooth：仅调整输入图像可能会导致对特定概念的 T2I 模型过度拟合的风险，为了解决这个问题，本文提出了特定于类的先验保留损失。这种方法的核心是使用预训练的 T2I 模型来生成与目标定制概念相同的类图像。然后，该模型在输入图像（使用 [V] 令牌）和模型生成的图像（没有 [V] 令牌）上联合微调。确保了模型能够在独特的“[V] 狗”和它最初训练的其他一般狗之间进行区分，同时保持其整体的T2I能力。\nAn image is worth one word: Personalizing text-to-image generation using textual inversion\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.\n多概念定制: 允许扩展文本到图像模型的Token词汇表，以包括多个概念Tokens，这使得多个概念能够在生成过程中相互作用以及与剩余的视觉场景交互。\nCustom Diffusion\nMulti-concept customization of text-to-image diffusion.\n个性化测试微调的简化: 由于测试时微调要求用户为每个新概念定制T2I模型，这可能会变得相当复杂。为简化使用流程，一些研究探索了无需测试时微调的定制方法，采用统一的微调阶段来扩展T2I模型，使其接受图像条件输入。这些模型将视觉概念的图像作为额外的输入条件，并根据文本描述生成包含视觉概念的图像。\nSuTI：训练单个模型来模拟微调的主题特定专家，并生成以文本和主题输入图像为条件的图像。 Subject-driven text-to-image generation via apprenticeship learning\n展望 Unified image and text inputs（统一图像文本输入） Tuning with alignment-focused loss and rewards（聚焦于对齐的损失和奖励） Closed-loop of multimodal content understanding and generation（多模态内容理解和生成的闭环） Q4: how to train multimodal LLM? Image-to-Text Generative Models 模型架构 预训练图像编码器与语言模型：这是多模态模型的基础。图像编码器（通常为基于 CNN 或 Transformer 的结构）学习提取图像的视觉特征，而语言模型（如 GPT 系列）负责建模文本序列的内在规律。 连接两个模态的可训练模块：为了融合视觉与语言信息，模型需要有专门的可训练模块。常见的有： Cross-Attentional Mechanisms： 跨模态注意力机制让图像特征与文本相互影响，赋予模型理解图文联系的能力。 Multimodal Fusion Modules： 多模态融合模块负责将来自不同模态的信息聚合，形成统一的表示。 训练目标 Cross-Attended Image-to-Text Generation：模型通过跨模态注意力，逐步生成与图像对应的文本描述。这是许多图像描述生成模型的核心目标。 Autoregressive loss on language output：自回归损失用于训练语言模型的生成能力。通过最小化预测的下一个词与真实词之间的差异，使模型学习生成流畅、自然的文本序列。 Large Multimodal Models LLaVA: Large Language-and-Vision Assistant 总结 Q5: how to chain vision experts with LLM? LLM + 各种工具\nEvolution of Modeling Paradigm New Paradigm MM-ReAct: Prompting ChatGPT for Multimodal Reasoning and Action\n","permalink":"/posts/recent-advances-in-vision-foundation-models.assets/","summary":"原文链接\nQ1: how to learn image representations? Overview 改进 CLIP 数据层面：Data scaling up 模型层面：Model design image side\nFLIP（Scaling CLIP training via masking）：是一种改进的训练方法，用于提高 CLIP模型的训练效率。FLIP 的核心思想是在训练过程中随机遮挡图像的部分区域，只对可见的区域进行编码。\nScaling language-image pre-training via masking, CVPR 2023\nlanguage side\nK-Lite: 将外部知识融入到对比学习预训练中，在 K-Lite 中，实体的维基百科定义（knowledge）可以与原始的图像替代文本（alt-text）一起自然地用于对比预训练。\nK-lite: Learning transferable visual models with external knowledge, NeurIPS 2022\nimproved interpretability\nSTAIR（Learning Sparse Text and Image Representation in Grounded Tokens）： 将图像和文本映射到高维稀疏嵌入空间； 每个维度的值是一个非负标量，表示与该维度对应的词或标记的权重； 提供了更好的性能和更清晰地识别图像和文本之间的对应关系； STAIR: Learning Sparse Text and Image Representation in Grounded Tokens, 2023","title":"Recent Advances in Vision Foundation Models"},{"content":"正向代理 正向代理是客户端和服务器中间的服务器，为了从原始服务器取得内容，客户端向代理服务器发送一个请求并指定目标（原始服务器）， 然后代理服务器向原始服务器转发请求并将获得的内容返回给客户端。\n举个例子，比如正常情况下我没法办上youtube，但是我有个aws的机器，它不受GFW的限制，能够访问youtube，我也能正常访问那个aws的机器， 那么我通过发送请求给aws的机器，让他转发我的请求给youtube，然后把youtube返回的数据给我，我就能通过aws的机器作为跳板访问GFW， 那个aws的机器也就是代理服务器的角色，并且这种方式就是正向代理。\n总结一下，正向代理就是我想访问一台机器，但是被墙了访问不到，我需要一台机器作为跳板转发我的请求。\n反向代理 于正向代理不同，反向代理更多的是为了保护原始服务器。 对于客户端而言，反向代理中的代理服务器就是原始服务器，客户端并不需要知道有这个代理的角色存在， 因此客户端也不需要一些额外的设置，比如正向代理中制定代理服务器是谁。\n比如在原始服务器A上配置防火墙，使得只有服务器B能够访问A，并且通过B服务器转发A的数据实现于外界的通信。这样对于客户端，它只需要和B交互， 从而隐藏了服务器A，B服务器也就是反向代理服务器。由于有代理服务器的存在，对于后面的原始服务器来说，也就多了一层做负载均衡的服务器。\n透明代理 还有一种代理方式叫做透明代理，比如公司的机器不能上qq，这个就是透明代理，它在内网和外网之间捕捉用户的请求，过滤一部分请求。\nSSH端口转发 有一种很简单的方式就能做到代理功能，那就是SSH本身提供的端口转发功能。 要想理解清楚SSH端口转发，首先必须记住这样几个原则：\nSSH简单的理解就是2台机器之间安全的数据通道，它包括ssh的client和ssh的server2个角色，这样的一条通道也就是ssh隧道(ssh tunneling) SSH 端口转发自然需要 SSH 连接，而 SSH 连接是有方向的，从 SSH Client 到 SSH Server 我们的应用的请求也是有方向的，一般是客户端向服务器端发出请求 一旦这2个方向相同，我们称为ssh的本地转发(-L)，不同则为远端转发(-R) 命令一般是跑在ssh client的机器上的 本地转发 ssh本地转发命令为：\nssh -L \u0026lt;local port\u0026gt;:\u0026lt;remote host\u0026gt;:\u0026lt;remote port\u0026gt; \u0026lt;SSH hostname\u0026gt; 这条命令可以翻译成：从本地的端口发出请求，通过这台机器作为跳板转发请求到的端口。 是相对而言的，比如是127.0.0.1的话，就是本身。 所以一般如果是127.0.0.1的话，跳板机或者代理服务器就是目标服务器。\n举个例子：\nA是一台在我家的机器macbook air，它可以访问taobao，也就是服务器C B是一台在公司的机器imac，由于在公司的内网，所以在家的A访问不到B C是taobao的服务器，公司不让上taobao，所以服务器B访问不了C 现在要想在让B服务器能访问C的80端口，由于防火墙这条路本身是走不通的，但是由于B能访问A，A能访问C，所以能把A作为代理服务器实现这一要求。\n在服务器B和服务器A之间建立ssh隧道，在SSH端口转发中，由于服务器B能连接到服务器A，并且请求是从服务器B发出，所以B既是ssh的client，也是请求的客户端 所以此时应该在B上去运行ssh的本地转发命令: ssh -L 8080:HOST_C:80 HOST_A\n远端转发 ssh远端转发的命令为：\nssh -R \u0026lt;local port\u0026gt;:\u0026lt;remote host\u0026gt;:\u0026lt;remote port\u0026gt; \u0026lt;SSH hostname\u0026gt; 由于本身B是可以访问A的，但是A访问不到B，现在要想服务器A访问到B，也就是在家能连上公司的机器。\n服务器B能连上A，所以B应该是ssh的client，此时请求是由A发起的，所以A是请求客户端，方向不同，所以是远端转发。 因此在B上运行命令: ssh -R 2222:127.0.0.1:22 HOST_A ,这里127.0.0.1是因为通过A服务器转发的目的服务器就是A本身。 这样在A上的2222端口就映射到了B的22端口。\n动态转发 不管是本地转发还是远端转发，都需要一个具体的应用服务器的地址和端口号，要想访问其他机器的内容就得绑定很多条这样的转发命令， 通过动态转发就能省去这一个应用服务器的信息。\nssh -D \u0026lt;local port\u0026gt; \u0026lt;SSH Server\u0026gt; 当我们在一个不安全的 WiFi 环境下上网，用 SSH 动态转发来保护我们的网页浏览等信息无疑是十分必要的。 比如在本机运行： sh -D 7001 \u0026lt;SSH Server\u0026gt; 这样就相当于通过创建了一个SOCKS代理。\n我们可以直接使用localhost:7001 来作为正常的 SOCKS 代理来使用，直接在浏览器上设置即可。 在 SSH Client 端无法访问的网站现在也都可以正常浏览。 而这里需要值得注意的是，此时 SSH 所包护的范围只包括从浏览器端（SSH Client 端）到 SSH Server 端的连接，并不包含从 SSH Server 端 到目标网站的连接。 如果后半截连接的安全不能得到充分的保证的话，这种方式仍不是合适的解决方案。\n这个时候还可以在本机将SOCKS代理转成HTTP代理。 比如在本地安装polipo，修改polipo.conf文件，把SOCKS代理填上127.0.0.1:7001，然后 export http_proxy=\u0026quot;127.0.0.1:8123\u0026quot; \u0026amp;\u0026amp; export https_proxy=\u0026quot;127.0.0.1:8123\u0026quot; 就在本地的8123端口起了一个http代理。\n","permalink":"/posts/proxy/","summary":"正向代理 正向代理是客户端和服务器中间的服务器，为了从原始服务器取得内容，客户端向代理服务器发送一个请求并指定目标（原始服务器）， 然后代理服务器向原始服务器转发请求并将获得的内容返回给客户端。\n举个例子，比如正常情况下我没法办上youtube，但是我有个aws的机器，它不受GFW的限制，能够访问youtube，我也能正常访问那个aws的机器， 那么我通过发送请求给aws的机器，让他转发我的请求给youtube，然后把youtube返回的数据给我，我就能通过aws的机器作为跳板访问GFW， 那个aws的机器也就是代理服务器的角色，并且这种方式就是正向代理。\n总结一下，正向代理就是我想访问一台机器，但是被墙了访问不到，我需要一台机器作为跳板转发我的请求。\n反向代理 于正向代理不同，反向代理更多的是为了保护原始服务器。 对于客户端而言，反向代理中的代理服务器就是原始服务器，客户端并不需要知道有这个代理的角色存在， 因此客户端也不需要一些额外的设置，比如正向代理中制定代理服务器是谁。\n比如在原始服务器A上配置防火墙，使得只有服务器B能够访问A，并且通过B服务器转发A的数据实现于外界的通信。这样对于客户端，它只需要和B交互， 从而隐藏了服务器A，B服务器也就是反向代理服务器。由于有代理服务器的存在，对于后面的原始服务器来说，也就多了一层做负载均衡的服务器。\n透明代理 还有一种代理方式叫做透明代理，比如公司的机器不能上qq，这个就是透明代理，它在内网和外网之间捕捉用户的请求，过滤一部分请求。\nSSH端口转发 有一种很简单的方式就能做到代理功能，那就是SSH本身提供的端口转发功能。 要想理解清楚SSH端口转发，首先必须记住这样几个原则：\nSSH简单的理解就是2台机器之间安全的数据通道，它包括ssh的client和ssh的server2个角色，这样的一条通道也就是ssh隧道(ssh tunneling) SSH 端口转发自然需要 SSH 连接，而 SSH 连接是有方向的，从 SSH Client 到 SSH Server 我们的应用的请求也是有方向的，一般是客户端向服务器端发出请求 一旦这2个方向相同，我们称为ssh的本地转发(-L)，不同则为远端转发(-R) 命令一般是跑在ssh client的机器上的 本地转发 ssh本地转发命令为：\nssh -L \u0026lt;local port\u0026gt;:\u0026lt;remote host\u0026gt;:\u0026lt;remote port\u0026gt; \u0026lt;SSH hostname\u0026gt; 这条命令可以翻译成：从本地的端口发出请求，通过这台机器作为跳板转发请求到的端口。 是相对而言的，比如是127.0.0.1的话，就是本身。 所以一般如果是127.0.0.1的话，跳板机或者代理服务器就是目标服务器。\n举个例子：\nA是一台在我家的机器macbook air，它可以访问taobao，也就是服务器C B是一台在公司的机器imac，由于在公司的内网，所以在家的A访问不到B C是taobao的服务器，公司不让上taobao，所以服务器B访问不了C 现在要想在让B服务器能访问C的80端口，由于防火墙这条路本身是走不通的，但是由于B能访问A，A能访问C，所以能把A作为代理服务器实现这一要求。\n在服务器B和服务器A之间建立ssh隧道，在SSH端口转发中，由于服务器B能连接到服务器A，并且请求是从服务器B发出，所以B既是ssh的client，也是请求的客户端 所以此时应该在B上去运行ssh的本地转发命令: ssh -L 8080:HOST_C:80 HOST_A\n远端转发 ssh远端转发的命令为：\nssh -R \u0026lt;local port\u0026gt;:\u0026lt;remote host\u0026gt;:\u0026lt;remote port\u0026gt; \u0026lt;SSH hostname\u0026gt; 由于本身B是可以访问A的，但是A访问不到B，现在要想服务器A访问到B，也就是在家能连上公司的机器。","title":"About proxy"},{"content":"Fluent Python Owner: Huijie Liu Tags: Ongoing, Study Note\n数据结构 序列构成的数组 Python 标准库用 C 实现了丰富的序列类型，列举如下。\n容器序列 list、tuple 和 collections.deque 这些序列能存放不同类型的数据。 扁平序列 str、bytes、bytearray、memoryview 和 array.array，这类序列只能容纳一种类型。 容器序列存放的是它们所包含的任意类型的对象的引用，而扁平序列里存放的是值而不是 引用。换句话说，扁平序列其实是一段连续的内存空间。由此可见扁平序列其实更加紧 凑，但是它里面只能存放诸如字符、字节和数值这种基础类型。\n序列类型还能按照能否被修改来分类。\n可变序列 list、bytearray、array.array、collections.deque 和 memoryview。 不可变序列 tuple、str 和 bytes。 字典和集合 💡 dict 类型是 Python 语言的基石。模块的命名空间、 实例的属性和函数的关键字参数中都可以看到字典的身影。跟它有关的内置函数都在 __builtins__.__dict__ 模块中。 正是因为字典至关重要，Python 对它的实现做了高度优化，而散列表则是字典类型性能出 众的根本原因。集合(set)的实现其实也依赖于散列表。\n如果一个对象是可散列的，那么在这个对象的生命周期中，它的散列值是不变的，而且这个对象需要实现 hash() 方法。另外可散列对象还要有 eq() 方法，这样才能跟其他键做比较。如果两个可散列对象是相等的，那么它们的散列值一定是一样的。一般来讲用户自定义的类型的对象都是可散列的。\n文本和字节序列 把函数视作对象 💡 可以把函数赋值给变量、传给 其他函数、存储在数据结构中，以及访问函数的属性，供框架和一些工具使用。 一等函数 在 Python 中，函数是一等对象。编程语言理论家把“**一等对象”**定义为满足下述条件的程 序实体:\n在运行时创建 能赋值给变量或数据结构中的元素 能作为参数传给函数 能作为函数的返回结果 在 Python 中，整数、字符串和字典都是一等对象。\n高阶函数 接受函数为参数，或者把函数作为结果返回的函数是高阶函数。函数式语言通常会提供 map、filter 和 reduce 三个高阶函数。\n函数内省 除了 doc，函数对象还有很多属性。使用 dir 函数可以探知所有属性:\n调用函数时使用 * 和 **“展开”可迭代对象\n函数注解 函数声明中的各个参数可以在 : 之后增加注解表达式。如果参数有默认值，注解放在参数名和 = 号之间。如果想注解返回值，在 ) 和函数声明末尾的 : 之间添加 -\u0026gt; 和一个表达式。 那个表达式可以是任何类型。\n函数装饰器和闭包 函数装饰器用于在源码中**“标记”**函数，以某种方式增强函数的行为。\n装饰器基础 装饰器是可调用的对象，其参数是另一个函数。装饰器可能会处理被装饰的函数，然后把它返回，或者将其替换成另一个函数或可调用对象。\n特性1，能把被装饰的函数替换成其他函数。 特性2，装饰器在加载模块时立即执行。（被装饰的函数定义之后立即运行） 闭包 闭包指延伸了作用域的函数，能访问定义体之外定义的非全局变量。闭包是一种函数，它会保留定义函数时存在的自由变量的绑定，这样调用函数时， 虽然定义作用域不可用了，但是仍能使用那些绑定。\naverager 的闭包延伸到那个函数的作用域之外，包含自由变量 series 的绑定\n在 averager 函数中，series 是自由变量，指未在本地作用域中绑定的变量。\nnonlocal 声明：它的作用是把变量标记为自由变量， 即使在函数中为变量赋予新值了，也会变成自由变量。\n面向对象惯用法 对象引用、可变性和垃圾回收 💡 - 每个 Python 对象都有标识、类型和值。只有对象的值会不时变化。 - 变量保存的是引用，这一点对 Python 编程有很多实际的影响。 - 对象的引用数量归零后，对象会被立即销毁。 在==和is之间选择 == 运算符比较两个对象的值(对象中保存的数据)，而 is 比较对象的标识（即引用）。\na == b是语法糖，等同于a.eq(b)。继承自object的eq 方法比较两个对象的 ID，结果与 is 一样。但是多数内置类型使用更有意义的方式覆盖了eq 方法，会考虑对象属性的值。\n元组的相对不可变性 元组与多数 Python 集合(列表、字典、集，等等)一样，保存的是对象的引用。如果引用的元素是可变的，即便元组本身不可变，元素依然可变。\n深复制和浅复制 浅复制：即复制了最外层容器，副本中的元素是源容器中元素的引用 深复制：即副本不共享内部对象的引用 函数的参数作为引用时 共享传参：函数的各个形式参数获得实参中各个引用的副本。也就是说，函数内部的形参是实参的别名。因此，函数可能会修改作为参数传入的可变对象，但是无法修改那些对象的 标识(即不能把一个对象替换成另一个对象)。\n💡 默认值在定义函数时计算(通常在加载模块时)，因此默认值变成了函数对象的属性。因此，如果默认值是可变对象，而且修改了它的值，那么后续的函数调用都会受到影响。 del、垃圾回收和弱引用 垃圾回收使用的主要算法是引用计数，当引用计数归零时，对象立即就被销毁。 弱引用不会增加对象的引用数量。引用的目标对象称为所指对象(referent)。因此弱引用不会妨碍所指对象被当作垃圾回收。 符合Python风格的对象 ","permalink":"/posts/fluent-python/","summary":"Fluent Python Owner: Huijie Liu Tags: Ongoing, Study Note\n数据结构 序列构成的数组 Python 标准库用 C 实现了丰富的序列类型，列举如下。\n容器序列 list、tuple 和 collections.deque 这些序列能存放不同类型的数据。 扁平序列 str、bytes、bytearray、memoryview 和 array.array，这类序列只能容纳一种类型。 容器序列存放的是它们所包含的任意类型的对象的引用，而扁平序列里存放的是值而不是 引用。换句话说，扁平序列其实是一段连续的内存空间。由此可见扁平序列其实更加紧 凑，但是它里面只能存放诸如字符、字节和数值这种基础类型。\n序列类型还能按照能否被修改来分类。\n可变序列 list、bytearray、array.array、collections.deque 和 memoryview。 不可变序列 tuple、str 和 bytes。 字典和集合 💡 dict 类型是 Python 语言的基石。模块的命名空间、 实例的属性和函数的关键字参数中都可以看到字典的身影。跟它有关的内置函数都在 __builtins__.__dict__ 模块中。 正是因为字典至关重要，Python 对它的实现做了高度优化，而散列表则是字典类型性能出 众的根本原因。集合(set)的实现其实也依赖于散列表。\n如果一个对象是可散列的，那么在这个对象的生命周期中，它的散列值是不变的，而且这个对象需要实现 hash() 方法。另外可散列对象还要有 eq() 方法，这样才能跟其他键做比较。如果两个可散列对象是相等的，那么它们的散列值一定是一样的。一般来讲用户自定义的类型的对象都是可散列的。\n文本和字节序列 把函数视作对象 💡 可以把函数赋值给变量、传给 其他函数、存储在数据结构中，以及访问函数的属性，供框架和一些工具使用。 一等函数 在 Python 中，函数是一等对象。编程语言理论家把“**一等对象”**定义为满足下述条件的程 序实体:\n在运行时创建 能赋值给变量或数据结构中的元素 能作为参数传给函数 能作为函数的返回结果 在 Python 中，整数、字符串和字典都是一等对象。","title":"Fluent Python"},{"content":"Chat with Me Loading... Send ","permalink":"/faq/","summary":"faq","title":"FAQ"}]